{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let:\n",
    "\n",
    "$fn$ = Number of features\n",
    "\n",
    "$hs$ = Number of output nodes (hidden size)\n",
    "\n",
    "$bs$ = Batch size\n",
    "\n",
    "Then:\n",
    " * Each $W_{something}$ matrix below has the shape $(fn, hs)$;\n",
    " * Each $U_{something}$ matrix below has the shape $(hs, hs)$;\n",
    " * Each $b_{something}$ matrix below has the shape $(1, hs)$;\n",
    " * The $x_t$ matrix below has shape $(bs, fn)$, corresponding to the element of index $t$ of each sequence inf the batch.; and\n",
    " * The $h_t$ matrix below has shape $(bs, hs)$, corresponding to hidden state at time $t$ of each sequence inf the batch.\n",
    "\n",
    "And:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$f_t = \\sigma(W_f \\ x_t + U_f \\ h_{t-1} + b_f)$\n",
    "\n",
    "$i_t = \\sigma(W_i \\ x_t + U_i \\ h_{t-1} + b_i)$\n",
    "\n",
    "$o_t = \\sigma(W_o \\ x_t + U_o \\ h_{t-1} + b_o)$\n",
    "\n",
    "$g_t = \\tanh \\ (W_g \\ x_t + U_g \\ h_{t-1} + b_g)$ a.k.a. $\\tilde{c}_t$\n",
    "\n",
    "$c_t = f_t \\circ c_{t-1} + i_t \\circ g_t$\n",
    "\n",
    "$h_t = o_t \\circ \\tanh \\ (c_t)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.w_f = nn.Parameter(torch.Tensor(input_size,hidden_size))\n",
    "        self.u_f= nn.Parameter(torch.Tensor(hidden_size,hidden_size))\n",
    "        self.b_f = nn.Parameter(torch.Tensor(1,hidden_size))\n",
    "        self.w_i = nn.Parameter(torch.Tensor(input_size,hidden_size))\n",
    "        self.u_i= nn.Parameter(torch.Tensor(hidden_size,hidden_size))\n",
    "        self.b_i = nn.Parameter(torch.Tensor(1,hidden_size))\n",
    "        self.w_o = nn.Parameter(torch.Tensor(input_size,hidden_size))\n",
    "        self.u_o= nn.Parameter(torch.Tensor(hidden_size,hidden_size))\n",
    "        self.b_o = nn.Parameter(torch.Tensor(1,hidden_size))\n",
    "        self.w_c = nn.Parameter(torch.Tensor(input_size,hidden_size))\n",
    "        self.u_c= nn.Parameter(torch.Tensor(hidden_size,hidden_size))\n",
    "        self.b_c = nn.Parameter(torch.Tensor(1,hidden_size))\n",
    "        self.init_weights()\n",
    "    def init_weights(self):\n",
    "        stdv = 1.0/math.sqrt(self.hidden_size)\n",
    "        for weight in self.parameters():\n",
    "            weight.data.uniform_(-stdv,stdv)\n",
    "\n",
    "    def forward(self, x, init_states = None):\n",
    "        bs,seq,_ = x.size()\n",
    "        if not init_states:\n",
    "            h_t = torch.zeros(bs, self.hidden_size) \n",
    "            c_t = torch.zeros(bs, self.hidden_size)\n",
    "        else:\n",
    "            h_t,c_t  = init_states\n",
    "            \n",
    "        hidden_seq = []\n",
    "        for t in range(seq):\n",
    "            x_t = x[:,t,:]\n",
    "            f_t = torch.sigmoid(x_t @ self.w_f + h_t @ self.u_f + self.b_f)\n",
    "            i_t = torch.sigmoid(x_t @ self.w_i + h_t @ self.u_i + self.b_i)\n",
    "            o_t = torch.sigmoid(x_t @ self.w_o + h_t @ self.u_o + self.b_o)\n",
    "            g_t = torch.tanh(x_t @ self.w_c + h_t @ self.u_c + self.b_c)\n",
    "            c_t = f_t * c_t + i_t * g_t\n",
    "            h_t = o_t * torch.tanh(c_t)\n",
    "            hidden_seq.append(h_t.unsqueeze(0))\n",
    "        hidden_seq = torch.cat(hidden_seq,dim=0)\n",
    "        hidden_seq = hidden_seq.transpose(0,1).contiguous()\n",
    "        return hidden_seq, (h_t,c_t)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self,input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.w_ih = nn.Parameter(torch.Tensor(input_size,hidden_size))\n",
    "        self.w_hh = nn.Parameter(torch.Tensor(hidden_size,hidden_size))\n",
    "        self.b_h = nn.Parameter(torch.Tensor(1,hidden_size))\n",
    "        self.init_weights()\n",
    "    def init_weights(self):\n",
    "        stdev = 1.0/math.sqrt(self.hidden_size)\n",
    "        for weight in self.parameters():\n",
    "            weight.data.uniform_(-stdev,stdev)\n",
    "    def forward(self, x, init_states= None):\n",
    "        batch_size,seq,_ = x.size()\n",
    "        if not init_states:\n",
    "            h_t = torch.zeros(batch_size,self.hidden_size)\n",
    "        else:\n",
    "            h_t = init_states\n",
    "        hidden_seqs= []\n",
    "        for t in range(seq):\n",
    "            x_t  = x[:,t,:]\n",
    "            h_t = torch.tanh(x_t @ self.w_ih + h_t @ self.w_hh + self.b_h)\n",
    "            hidden_seqs.append(h_t.unsqueeze(0))\n",
    "        hidden_seqs = torch.cat(hidden_seqs,dim=0)\n",
    "        hidden_seqs = hidden_seqs.transpose(0,1).contiguous()\n",
    "        return hidden_seqs, h_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = LSTM(30,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = RNN(30,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.0383, -0.0926,  0.5642,  0.0537, -0.0438],\n",
       "          [ 0.0467, -0.0919,  0.7110,  0.0895, -0.0592],\n",
       "          [ 0.0499, -0.0883,  0.7483,  0.0999, -0.0666],\n",
       "          [ 0.0510, -0.0867,  0.7611,  0.1023, -0.0699],\n",
       "          [ 0.0514, -0.0861,  0.7666,  0.1030, -0.0714]]],\n",
       "        grad_fn=<TransposeBackward0>),\n",
       " (tensor([[ 0.0514, -0.0861,  0.7666,  0.1030, -0.0714]], grad_fn=<MulBackward0>),\n",
       "  tensor([[ 0.2105, -0.4219,  2.2219,  0.5070, -0.4391]], grad_fn=<AddBackward0>)))"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = torch.ones((1, 5, 30))\n",
    "lstm(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.7895, -0.8852, -0.9403, -0.9718,  0.3570],\n",
       "          [ 0.9516, -0.8796, -0.9175, -0.9619, -0.0694],\n",
       "          [ 0.9428, -0.9149, -0.8845, -0.9634, -0.0624],\n",
       "          [ 0.9449, -0.9145, -0.8886, -0.9648, -0.0710],\n",
       "          [ 0.9446, -0.9151, -0.8877, -0.9648, -0.0699]]],\n",
       "        grad_fn=<TransposeBackward0>),\n",
       " tensor([[ 0.9446, -0.9151, -0.8877, -0.9648, -0.0699]], grad_fn=<TanhBackward>))"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = torch.ones((1, 5, 30))\n",
    "rnn(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lstm_layer = RNN(30,5)\n",
    "        self.fc1 = nn.Linear(5,3)\n",
    "    def forward(self, x):\n",
    "        h_s,_ = self.lstm_layer(x)\n",
    "        y = self.fc1(h_s[:,-1,:])\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4908,  0.4611, -1.1691]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9302617907524109\n",
      "0.988351047039032\n",
      "1.1443642377853394\n",
      "0.9165745973587036\n",
      "1.0967761278152466\n",
      "test accuracy: 0.3333333333333333\n",
      "0.8754047751426697\n",
      "0.9321969151496887\n",
      "1.119646430015564\n",
      "0.8730838298797607\n",
      "1.079025149345398\n",
      "test accuracy: 0.6666666666666666\n",
      "0.8516170978546143\n",
      "0.8815810680389404\n",
      "1.0946593284606934\n",
      "0.8311192393302917\n",
      "1.0590115785598755\n",
      "test accuracy: 0.6666666666666666\n",
      "0.8286897540092468\n",
      "0.8322532176971436\n",
      "1.0703446865081787\n",
      "0.7902989983558655\n",
      "1.0384382009506226\n",
      "test accuracy: 0.6666666666666666\n",
      "0.8037112951278687\n",
      "0.7837380170822144\n",
      "1.0462753772735596\n",
      "0.7504451274871826\n",
      "1.0172709226608276\n",
      "test accuracy: 0.6666666666666666\n",
      "0.775713324546814\n",
      "0.7357447147369385\n",
      "1.0217498540878296\n",
      "0.7113040685653687\n",
      "0.9950616955757141\n",
      "test accuracy: 0.6666666666666666\n",
      "0.7443580627441406\n",
      "0.6880583167076111\n",
      "0.9960266351699829\n",
      "0.6726559400558472\n",
      "0.9712073802947998\n",
      "test accuracy: 0.6666666666666666\n",
      "0.709659218788147\n",
      "0.6405701637268066\n",
      "0.9682766795158386\n",
      "0.6343910694122314\n",
      "0.9449901580810547\n",
      "test accuracy: 0.6666666666666666\n",
      "0.6718997955322266\n",
      "0.5933078527450562\n",
      "0.9375572800636292\n",
      "0.5964776873588562\n",
      "0.9156233072280884\n",
      "test accuracy: 0.6666666666666666\n",
      "0.6315874457359314\n",
      "0.5464356541633606\n",
      "0.9028169512748718\n",
      "0.5588854551315308\n",
      "0.8823145031929016\n",
      "test accuracy: 0.6666666666666666\n",
      "0.5894071459770203\n",
      "0.5002294182777405\n",
      "0.8629957437515259\n",
      "0.5215533375740051\n",
      "0.8443543910980225\n",
      "test accuracy: 0.6666666666666666\n",
      "0.5461643934249878\n",
      "0.45504850149154663\n",
      "0.8172943592071533\n",
      "0.4844224452972412\n",
      "0.8012488484382629\n",
      "test accuracy: 0.6666666666666666\n",
      "0.502733588218689\n",
      "0.41131165623664856\n",
      "0.7655251026153564\n",
      "0.447471559047699\n",
      "0.7529309988021851\n",
      "test accuracy: 0.6666666666666666\n",
      "0.46001455187797546\n",
      "0.3694770634174347\n",
      "0.7083670496940613\n",
      "0.41072672605514526\n",
      "0.6999974846839905\n",
      "test accuracy: 0.6666666666666666\n",
      "0.41888222098350525\n",
      "0.330021470785141\n",
      "0.6474161744117737\n",
      "0.3742850124835968\n",
      "0.6437588334083557\n",
      "test accuracy: 0.6666666666666666\n",
      "0.3801092207431793\n",
      "0.29340705275535583\n",
      "0.5849463939666748\n",
      "0.3383753001689911\n",
      "0.5860515832901001\n",
      "test accuracy: 0.6666666666666666\n",
      "0.34426379203796387\n",
      "0.2600339949131012\n",
      "0.523419201374054\n",
      "0.3034270405769348\n",
      "0.5288727283477783\n",
      "test accuracy: 0.6666666666666666\n",
      "0.311636745929718\n",
      "0.23018069565296173\n",
      "0.4649641215801239\n",
      "0.27006450295448303\n",
      "0.4739972949028015\n",
      "test accuracy: 0.6666666666666666\n",
      "0.2822573184967041\n",
      "0.20395250618457794\n",
      "0.41106393933296204\n",
      "0.23899510502815247\n",
      "0.4227466583251953\n",
      "test accuracy: 0.6666666666666666\n",
      "0.255993515253067\n",
      "0.18126395344734192\n",
      "0.3625100255012512\n",
      "0.2108379453420639\n",
      "0.37593844532966614\n",
      "test accuracy: 0.6666666666666666\n",
      "0.2326473891735077\n",
      "0.161863774061203\n",
      "0.31953227519989014\n",
      "0.1859741508960724\n",
      "0.333945631980896\n",
      "test accuracy: 0.6666666666666666\n",
      "0.21199758350849152\n",
      "0.14539015293121338\n",
      "0.28197571635246277\n",
      "0.16448776423931122\n",
      "0.2967958152294159\n",
      "test accuracy: 0.6666666666666666\n",
      "0.19380873441696167\n",
      "0.13143421709537506\n",
      "0.24945451319217682\n",
      "0.1462032049894333\n",
      "0.26427608728408813\n",
      "test accuracy: 0.6666666666666666\n",
      "0.1778336465358734\n",
      "0.11959104984998703\n",
      "0.22146368026733398\n",
      "0.130778506398201\n",
      "0.23602288961410522\n",
      "test accuracy: 0.6666666666666666\n",
      "0.16381947696208954\n",
      "0.10949238389730453\n",
      "0.19745536148548126\n",
      "0.11780274659395218\n",
      "0.21159468591213226\n",
      "test accuracy: 0.6666666666666666\n",
      "0.15151852369308472\n",
      "0.100821852684021\n",
      "0.176888108253479\n",
      "0.10686693340539932\n",
      "0.1905239373445511\n",
      "test accuracy: 0.6666666666666666\n",
      "0.14069898426532745\n",
      "0.09331774711608887\n",
      "0.1592578887939453\n",
      "0.09760364890098572\n",
      "0.17235426604747772\n",
      "test accuracy: 0.6666666666666666\n",
      "0.1311524659395218\n",
      "0.086768239736557\n",
      "0.14411239326000214\n",
      "0.08970218151807785\n",
      "0.15666283667087555\n",
      "test accuracy: 0.6666666666666666\n",
      "0.12269709259271622\n",
      "0.08100545406341553\n",
      "0.13105805218219757\n",
      "0.08290845900774002\n",
      "0.14307242631912231\n",
      "test accuracy: 0.6666666666666666\n",
      "0.11517801880836487\n",
      "0.07589590549468994\n",
      "0.11975900083780289\n",
      "0.07701870054006577\n",
      "0.1312560886144638\n",
      "test accuracy: 0.6666666666666666\n",
      "0.10846506059169769\n",
      "0.07133394479751587\n",
      "0.10993238538503647\n",
      "0.07187116146087646\n",
      "0.12093494087457657\n",
      "test accuracy: 0.6666666666666666\n",
      "0.1024494618177414\n",
      "0.06723602861166\n",
      "0.1013428121805191\n",
      "0.06733778119087219\n",
      "0.11187487840652466\n",
      "test accuracy: 0.6666666666666666\n",
      "0.09704060107469559\n",
      "0.06353458762168884\n",
      "0.09379520267248154\n",
      "0.06331653147935867\n",
      "0.10388114303350449\n",
      "test accuracy: 0.6666666666666666\n",
      "0.0921623557806015\n",
      "0.06017562001943588\n",
      "0.0871286541223526\n",
      "0.05972641706466675\n",
      "0.0967920646071434\n",
      "test accuracy: 0.6666666666666666\n",
      "0.0877503827214241\n",
      "0.05711433291435242\n",
      "0.08121012896299362\n",
      "0.05650198459625244\n",
      "0.09047401696443558\n",
      "test accuracy: 0.6666666666666666\n",
      "0.08374936133623123\n",
      "0.05431364104151726\n",
      "0.07592993974685669\n",
      "0.053590208292007446\n",
      "0.08481606096029282\n",
      "test accuracy: 0.6666666666666666\n",
      "0.08011151850223541\n",
      "0.051743052899837494\n",
      "0.07119739800691605\n",
      "0.05094808712601662\n",
      "0.079726442694664\n",
      "test accuracy: 0.6666666666666666\n",
      "0.07679461687803268\n",
      "0.049376342445611954\n",
      "0.06693705171346664\n",
      "0.04853951930999756\n",
      "0.07512877136468887\n",
      "test accuracy: 0.6666666666666666\n",
      "0.07376104593276978\n",
      "0.04719075560569763\n",
      "0.06308610737323761\n",
      "0.04633525758981705\n",
      "0.07095912843942642\n",
      "test accuracy: 0.6666666666666666\n",
      "0.07097667455673218\n",
      "0.045167550444602966\n",
      "0.059591878205537796\n",
      "0.04431008920073509\n",
      "0.06716380268335342\n",
      "test accuracy: 0.6666666666666666\n",
      "0.0684112161397934\n",
      "0.043289802968502045\n",
      "0.05641016364097595\n",
      "0.04244318604469299\n",
      "0.06369756907224655\n",
      "test accuracy: 0.6666666666666666\n",
      "0.0660371407866478\n",
      "0.04154336079955101\n",
      "0.053503427654504776\n",
      "0.04071643576025963\n",
      "0.06052195653319359\n",
      "test accuracy: 0.6666666666666666\n",
      "0.06383044272661209\n",
      "0.03991517424583435\n",
      "0.050839778035879135\n",
      "0.03911464288830757\n",
      "0.05760398507118225\n",
      "test accuracy: 0.6666666666666666\n",
      "0.06176994740962982\n",
      "0.03839423879981041\n",
      "0.04839199408888817\n",
      "0.03762451931834221\n",
      "0.054915592074394226\n",
      "test accuracy: 0.6666666666666666\n",
      "0.05983758717775345\n",
      "0.0369705930352211\n",
      "0.04613663628697395\n",
      "0.03623461350798607\n",
      "0.05243241786956787\n",
      "test accuracy: 0.6666666666666666\n",
      "0.05801837146282196\n",
      "0.03563545644283295\n",
      "0.044053349643945694\n",
      "0.0349348708987236\n",
      "0.05013321712613106\n",
      "test accuracy: 0.6666666666666666\n",
      "0.056299738585948944\n",
      "0.03438112139701843\n",
      "0.04212447628378868\n",
      "0.033716656267642975\n",
      "0.047999732196331024\n",
      "test accuracy: 0.6666666666666666\n",
      "0.054671380668878555\n",
      "0.03320051729679108\n",
      "0.04033466428518295\n",
      "0.03257230669260025\n",
      "0.046015623956918716\n",
      "test accuracy: 0.6666666666666666\n",
      "0.053124696016311646\n",
      "0.032087672501802444\n",
      "0.03867032751441002\n",
      "0.03149515017867088\n",
      "0.04416661337018013\n",
      "test accuracy: 0.6666666666666666\n",
      "0.05165271833539009\n",
      "0.031036803498864174\n",
      "0.037119701504707336\n",
      "0.030479270964860916\n",
      "0.042440444231033325\n",
      "test accuracy: 0.6666666666666666\n",
      "0.050249312072992325\n",
      "0.030043132603168488\n",
      "0.03567203879356384\n",
      "0.029519423842430115\n",
      "0.0408257320523262\n",
      "test accuracy: 0.6666666666666666\n",
      "0.04890954867005348\n",
      "0.029102087020874023\n",
      "0.034318115562200546\n",
      "0.028611132875084877\n",
      "0.03931271657347679\n",
      "test accuracy: 0.6666666666666666\n",
      "0.04762891307473183\n",
      "0.028209874406456947\n",
      "0.033049557358026505\n",
      "0.027750251814723015\n",
      "0.03789244592189789\n",
      "test accuracy: 0.6666666666666666\n",
      "0.04640331491827965\n",
      "0.027362681925296783\n",
      "0.03185906633734703\n",
      "0.02693307399749756\n",
      "0.0365571603178978\n",
      "test accuracy: 0.6666666666666666\n",
      "0.04522907733917236\n",
      "0.026557261124253273\n",
      "0.03073986992239952\n",
      "0.026156337931752205\n",
      "0.035299841314554214\n",
      "test accuracy: 0.6666666666666666\n",
      "0.04410319775342941\n",
      "0.025790920481085777\n",
      "0.029686076566576958\n",
      "0.02541711926460266\n",
      "0.03411410003900528\n",
      "test accuracy: 0.6666666666666666\n",
      "0.04302263632416725\n",
      "0.025060733780264854\n",
      "0.0286923386156559\n",
      "0.0247129425406456\n",
      "0.03299419581890106\n",
      "test accuracy: 0.6666666666666666\n",
      "0.04198480024933815\n",
      "0.024364449083805084\n",
      "0.027753962203860283\n",
      "0.024041203781962395\n",
      "0.031935159116983414\n",
      "test accuracy: 0.6666666666666666\n",
      "0.040987081825733185\n",
      "0.023699810728430748\n",
      "0.02686646394431591\n",
      "0.023399874567985535\n",
      "0.03093232586979866\n",
      "test accuracy: 0.6666666666666666\n",
      "0.040027305483818054\n",
      "0.023064669221639633\n",
      "0.026026146486401558\n",
      "0.022787030786275864\n",
      "0.029981357976794243\n",
      "test accuracy: 0.6666666666666666\n",
      "0.03910340741276741\n",
      "0.02245733141899109\n",
      "0.025229526683688164\n",
      "0.022200513631105423\n",
      "0.029078813269734383\n",
      "test accuracy: 0.6666666666666666\n",
      "0.03821330890059471\n",
      "0.021875984966754913\n",
      "0.024473220109939575\n",
      "0.021639198064804077\n",
      "0.028220999985933304\n",
      "test accuracy: 0.6666666666666666\n",
      "0.037355490028858185\n",
      "0.021319156512618065\n",
      "0.023754524067044258\n",
      "0.021101029589772224\n",
      "0.0274049025028944\n",
      "test accuracy: 0.6666666666666666\n",
      "0.0365280844271183\n",
      "0.020785368978977203\n",
      "0.023070842027664185\n",
      "0.020585110411047935\n",
      "0.02662760578095913\n",
      "test accuracy: 0.6666666666666666\n",
      "0.03572966530919075\n",
      "0.020273257046937943\n",
      "0.022419564425945282\n",
      "0.02008972130715847\n",
      "0.025886761024594307\n",
      "test accuracy: 0.6666666666666666\n",
      "0.03495858609676361\n",
      "0.019781455397605896\n",
      "0.021798772737383842\n",
      "0.01961395889520645\n",
      "0.025179658085107803\n",
      "test accuracy: 0.6666666666666666\n",
      "0.03421363607048988\n",
      "0.01930905319750309\n",
      "0.02120642177760601\n",
      "0.01915656588971615\n",
      "0.024504395201802254\n",
      "test accuracy: 0.6666666666666666\n",
      "0.033493608236312866\n",
      "0.018855029717087746\n",
      "0.020640579983592033\n",
      "0.018716629594564438\n",
      "0.023858822882175446\n",
      "test accuracy: 0.6666666666666666\n",
      "0.03279729187488556\n",
      "0.018418239429593086\n",
      "0.02009965293109417\n",
      "0.018293239176273346\n",
      "0.023241253569722176\n",
      "test accuracy: 0.6666666666666666\n",
      "0.03212357312440872\n",
      "0.01799764856696129\n",
      "0.019582046195864677\n",
      "0.0178853590041399\n",
      "0.02264975756406784\n",
      "test accuracy: 0.6666666666666666\n",
      "0.03147135302424431\n",
      "0.017592692747712135\n",
      "0.019086511805653572\n",
      "0.017492424696683884\n",
      "0.02208309806883335\n",
      "test accuracy: 0.6666666666666666\n",
      "0.030839743092656136\n",
      "0.017202338203787804\n",
      "0.01861167699098587\n",
      "0.017113396897912025\n",
      "0.02153956890106201\n",
      "test accuracy: 0.6666666666666666\n",
      "0.030227743089199066\n",
      "0.01682601496577263\n",
      "0.01815616525709629\n",
      "0.01674770936369896\n",
      "0.02101803757250309\n",
      "test accuracy: 0.6666666666666666\n",
      "0.02963458001613617\n",
      "0.016462918370962143\n",
      "0.017719069495797157\n",
      "0.01639466919004917\n",
      "0.02051714062690735\n",
      "test accuracy: 0.6666666666666666\n",
      "0.029059244319796562\n",
      "0.01611235737800598\n",
      "0.017299238592386246\n",
      "0.01605382189154625\n",
      "0.02003585733473301\n",
      "test accuracy: 0.6666666666666666\n",
      "0.028501303866505623\n",
      "0.01577387936413288\n",
      "0.016895757988095284\n",
      "0.015724359080195427\n",
      "0.019572928547859192\n",
      "test accuracy: 0.6666666666666666\n",
      "0.027959860861301422\n",
      "0.015446905978024006\n",
      "0.01650782860815525\n",
      "0.015405821613967419\n",
      "0.01912744529545307\n",
      "test accuracy: 0.6666666666666666\n",
      "0.027434131130576134\n",
      "0.01513062883168459\n",
      "0.016134411096572876\n",
      "0.015097633004188538\n",
      "0.018698612228035927\n",
      "test accuracy: 0.6666666666666666\n",
      "0.026923557743430138\n",
      "0.014824822545051575\n",
      "0.015774935483932495\n",
      "0.0147994514554739\n",
      "0.018285397440195084\n",
      "test accuracy: 0.6666666666666666\n",
      "0.026427701115608215\n",
      "0.0145289096981287\n",
      "0.015428476966917515\n",
      "0.014510580338537693\n",
      "0.017887232825160027\n",
      "test accuracy: 0.6666666666666666\n",
      "0.025945769622921944\n",
      "0.014242545701563358\n",
      "0.015094580128788948\n",
      "0.014231028035283089\n",
      "0.017503084614872932\n",
      "test accuracy: 0.6666666666666666\n",
      "0.025477202609181404\n",
      "0.013965151272714138\n",
      "0.014772552996873856\n",
      "0.013959978707134724\n",
      "0.017132380977272987\n",
      "test accuracy: 0.6666666666666666\n",
      "0.025021668523550034\n",
      "0.013696382753551006\n",
      "0.014461702667176723\n",
      "0.013697205111384392\n",
      "0.016774436458945274\n",
      "test accuracy: 0.6666666666666666\n",
      "0.02457837387919426\n",
      "0.013435891829431057\n",
      "0.014161568135023117\n",
      "0.013442478142678738\n",
      "0.016428792849183083\n",
      "test accuracy: 0.6666666666666666\n",
      "0.024147218093276024\n",
      "0.013183332979679108\n",
      "0.013871690258383751\n",
      "0.013195215724408627\n",
      "0.016094645485281944\n",
      "test accuracy: 0.6666666666666666\n",
      "0.02372751757502556\n",
      "0.012938478030264378\n",
      "0.01359148882329464\n",
      "0.012955423444509506\n",
      "0.01577153243124485\n",
      "test accuracy: 0.6666666666666666\n",
      "0.023319052532315254\n",
      "0.012700742110610008\n",
      "0.013320501893758774\n",
      "0.012722400017082691\n",
      "0.015458996407687664\n",
      "test accuracy: 0.6666666666666666\n",
      "0.02292102389037609\n",
      "0.012469896115362644\n",
      "0.013058265671133995\n",
      "0.01249626837670803\n",
      "0.0151565782725811\n",
      "test accuracy: 0.6666666666666666\n",
      "0.022533444687724113\n",
      "0.012245944701135159\n",
      "0.012804552912712097\n",
      "0.012276443652808666\n",
      "0.014863698743283749\n",
      "test accuracy: 0.6666666666666666\n",
      "0.02215585671365261\n",
      "0.012028303928673267\n",
      "0.012558897957205772\n",
      "0.012062814086675644\n",
      "0.014580016024410725\n",
      "test accuracy: 0.6666666666666666\n",
      "0.02178780920803547\n",
      "0.011816741898655891\n",
      "0.012320954352617264\n",
      "0.01185514684766531\n",
      "0.014305183663964272\n",
      "test accuracy: 0.6666666666666666\n",
      "0.02142907679080963\n",
      "0.01161137968301773\n",
      "0.012090256437659264\n",
      "0.011653091758489609\n",
      "0.014038621447980404\n",
      "test accuracy: 0.6666666666666666\n",
      "0.021079203113913536\n",
      "0.011411396786570549\n",
      "0.011866691522300243\n",
      "0.011456652544438839\n",
      "0.013780218549072742\n",
      "test accuracy: 0.6666666666666666\n",
      "0.020737964659929276\n",
      "0.011217032559216022\n",
      "0.011649792082607746\n",
      "0.01126547995954752\n",
      "0.013529511168599129\n",
      "test accuracy: 0.6666666666666666\n",
      "0.020405136048793793\n",
      "0.011027935892343521\n",
      "0.011439328081905842\n",
      "0.011079221963882446\n",
      "0.013286151923239231\n",
      "test accuracy: 0.6666666666666666\n",
      "0.02008037455379963\n",
      "0.010843755677342415\n",
      "0.011235185898840427\n",
      "0.010897999629378319\n",
      "0.013049911707639694\n",
      "test accuracy: 0.6666666666666666\n",
      "0.019763456657528877\n",
      "0.010664613917469978\n",
      "0.01103677786886692\n",
      "0.010721461847424507\n",
      "0.012820441275835037\n",
      "test accuracy: 0.6666666666666666\n",
      "0.01945391856133938\n",
      "0.010490158572793007\n",
      "0.010844227857887745\n",
      "0.010549374856054783\n",
      "0.012597509659826756\n",
      "test accuracy: 0.6666666666666666\n",
      "0.019151771441102028\n",
      "0.010320274159312248\n",
      "0.01065706554800272\n",
      "0.01038186065852642\n",
      "0.012380886822938919\n",
      "test accuracy: 0.6666666666666666\n",
      "0.01885678432881832\n",
      "0.010154726915061474\n",
      "0.010475176386535168\n",
      "0.01021844707429409\n",
      "0.012170223519206047\n",
      "test accuracy: 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "device = \"cpu\"\n",
    "classifier = Net().to(device)\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=0.005)#0.002 dives 85% acc\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "category = [0,1,2]\n",
    "data = [([1,2,3,4,5],0),\n",
    "     ([2,5,11,13,15],1),\n",
    "     ([12,15,22,24,25],2),\n",
    "     ([5,6,3,9,11],1),\n",
    "     ([23,15,26,27],2)]\n",
    "\n",
    "test_data = [([1,5,7,8,3],0),\n",
    "             ([11,23,14,16,17],1),\n",
    "             ([23,25,12,28,29],2)]\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    for x,y in data:\n",
    "        input_tensor = torch.zeros((1,5,30))\n",
    "        labels = torch.tensor([y], dtype=torch.long)\n",
    "        for i, elem in enumerate(x):\n",
    "            input_tensor[0][i][elem] = 1\n",
    "        optimizer.zero_grad()\n",
    "        preds = classifier(input_tensor.to(device))\n",
    "        loss = criterion(preds, labels.to(device))\n",
    "        print(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()  \n",
    "    acc = 0\n",
    "    with torch.no_grad():\n",
    "        for  x,y in test_data:\n",
    "            input_tensor = torch.zeros((1,5,30))\n",
    "            labels = torch.tensor([y], dtype=torch.long)\n",
    "            for i, elem in enumerate(x):\n",
    "                input_tensor[0][i][elem] = 1\n",
    "            preds = classifier(input_tensor.to(device))\n",
    "            acc += (preds.argmax(dim=1) == labels.to(device)).float().sum().cpu().item()\n",
    "    acc /= len(test_data)\n",
    "    print(\"test accuracy:\",acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.w_f = nn.Parameter(torch.Tensor(input_size,hidden_size))\n",
    "        self.u_f= nn.Parameter(torch.Tensor(hidden_size,hidden_size))\n",
    "        self.b_f = nn.Parameter(torch.Tensor(1,hidden_size))\n",
    "        self.w_i = nn.Parameter(torch.Tensor(input_size,hidden_size))\n",
    "        self.u_i= nn.Parameter(torch.Tensor(hidden_size,hidden_size))\n",
    "        self.b_i = nn.Parameter(torch.Tensor(1,hidden_size))\n",
    "        self.w_o = nn.Parameter(torch.Tensor(input_size,hidden_size))\n",
    "        self.u_o= nn.Parameter(torch.Tensor(hidden_size,hidden_size))\n",
    "        self.b_o = nn.Parameter(torch.Tensor(1,hidden_size))\n",
    "        self.w_c = nn.Parameter(torch.Tensor(input_size,hidden_size))\n",
    "        self.u_c= nn.Parameter(torch.Tensor(hidden_size,hidden_size))\n",
    "        self.b_c = nn.Parameter(torch.Tensor(1,hidden_size))\n",
    "        self.init_weights()\n",
    "    def init_weights(self):\n",
    "        stdv = 1.0/math.sqrt(self.hidden_size)\n",
    "        for weight in self.parameters():\n",
    "            weight.data.uniform_(-stdv,stdv)\n",
    "\n",
    "    def forward(self, x_t, h_t_c_t):\n",
    "        h_t,c_t = h_t_c_t\n",
    "        f_t = torch.sigmoid(x_t @ self.w_f + h_t @ self.u_f + self.b_f)\n",
    "        i_t = torch.sigmoid(x_t @ self.w_i + h_t @ self.u_i + self.b_i)\n",
    "        o_t = torch.sigmoid(x_t @ self.w_o + h_t @ self.u_o + self.b_o)\n",
    "        g_t = torch.tanh(x_t @ self.w_c + h_t @ self.u_c + self.b_c)\n",
    "        c_t = f_t * c_t + i_t * g_t\n",
    "        h_t = o_t * torch.tanh(c_t)\n",
    "        return (h_t,c_t)\n",
    "    def init_hidden_cell_state(self):\n",
    "        return torch.zeros(1,self.hidden_size),torch.zeros(1,self.hidden_size)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstmcell = LSTMCell(10,5)\n",
    "inp = torch.ones(1,10)\n",
    "h_t,c_t = lstmcell.init_hidden_cell_state()\n",
    "h_t,c_t = lstmcell.forward(inp,(h_t,c_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMNet(nn.Module):\n",
    "    def __init__(self, input_size,hidden_size,output_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.LSTMCell(input_size,hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size,output_size)\n",
    "    def forward(self, x_t, h_t, c_t):\n",
    "        h_t, c_t = self.rnn(x_t,(h_t, c_t))\n",
    "        o_t = self.fc(h_t)\n",
    "        return o_t, h_t, c_t \n",
    "    def init_hidden_cell_state(self):\n",
    "        return torch.zeros(1,self.hidden_size),torch.zeros(1,self.hidden_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = torch.ones(1,10)\n",
    "rnn = LSTMNet(10,5,2)\n",
    "h_t,c_t = rnn.init_hidden_cell_state()\n",
    "o_t,h_t,c_t = rnn(inp,h_t,c_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNCell(nn.Module):\n",
    "    \n",
    "    def __init__(self,input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.w_ih = nn.Parameter(torch.Tensor(input_size,hidden_size))\n",
    "        self.w_hh = nn.Parameter(torch.Tensor(hidden_size,hidden_size))\n",
    "        self.b_h = nn.Parameter(torch.Tensor(1,hidden_size))\n",
    "        self.init_weights()\n",
    "    def init_weights(self):\n",
    "        stdev = 1.0/math.sqrt(self.hidden_size)\n",
    "        for weight in self.parameters():\n",
    "            weight.data.uniform_(-stdev,stdev)\n",
    "    def init_hidden_state(self):\n",
    "        return torch.zeros(1,self.hidden_size)\n",
    "    def forward(self, x_t, h_t):\n",
    "        h_t = torch.tanh(x_t @ self.w_ih + h_t @ self.w_hh + self.b_h)\n",
    "        return h_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnncell = RNNCell(10,5)\n",
    "inp = torch.ones(1,10)\n",
    "hidden_state = rnncell.init_hidden_state()\n",
    "hidden_state = rnncell.forward(inp,hidden_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNNet(nn.Module):\n",
    "    def __init__(self, input_size,hidden_size,output_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.RNNCell(input_size,hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size,output_size)\n",
    "    def forward(self, x_t, h_t):\n",
    "        h_t = self.rnn(x_t,h_t)\n",
    "        o_t = self.fc(h_t)\n",
    "        return o_t, h_t \n",
    "    def init_hidden_state(self):\n",
    "        return torch.zeros(1,self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = torch.ones(1,10)\n",
    "rnn = RNNNet(10,5,2)\n",
    "h_t = rnn.init_hidden_state()\n",
    "o_t,h_t = rnn(inp,h_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_tensor(x):\n",
    "    tensor = torch.zeros(1,30)\n",
    "    tensor[0][x] = 1\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.278469721476237\n",
      "3.143273035685221\n",
      "3.224144617716471\n",
      "3.207681973775228\n",
      "3.072141965230306\n",
      "3.1923481623331704\n",
      "3.1395816802978516\n",
      "3.004084269205729\n",
      "3.1620969772338867\n",
      "3.074049631754557\n",
      "2.9389527638753257\n",
      "3.1332858403523765\n",
      "3.0109875996907554\n",
      "2.8766180674235025\n",
      "3.1058197021484375\n",
      "2.9503164291381836\n",
      "2.816965103149414\n",
      "3.0796111424764\n",
      "2.8919690450032554\n",
      "2.7598915100097656\n",
      "3.054576555887858\n",
      "2.8358866373697915\n",
      "2.705301602681478\n",
      "3.0306371053059897\n",
      "2.7820138931274414\n",
      "2.6531054178873696\n",
      "3.007713953653971\n",
      "2.7302964528401694\n",
      "2.6032158533732095\n",
      "2.9857301712036133\n",
      "2.6806774139404297\n",
      "2.5555454889933267\n",
      "2.9646072387695312\n",
      "2.63309637705485\n",
      "2.5100056330362954\n",
      "2.9442675908406577\n",
      "2.5874862670898438\n",
      "2.4665053685506186\n",
      "2.9246317545572915\n",
      "2.5437746047973633\n",
      "2.4249507586161294\n",
      "2.9056215286254883\n",
      "2.5018819173177085\n",
      "2.3852438926696777\n",
      "2.887157440185547\n",
      "2.461721738179525\n",
      "2.347283363342285\n",
      "2.869159698486328\n",
      "2.423201243082682\n",
      "2.3109636306762695\n",
      "2.8515491485595703\n",
      "2.386221249898275\n",
      "2.2761764526367188\n",
      "2.8342485427856445\n",
      "2.350677967071533\n",
      "2.242809454600016\n",
      "2.8171799977620444\n",
      "2.316462834676107\n",
      "2.210749944051107\n",
      "2.8002665837605796\n",
      "2.283463478088379\n",
      "2.179882208506266\n",
      "2.7834339141845703\n",
      "2.251566251118978\n",
      "2.150090217590332\n",
      "2.766606648763021\n",
      "2.220655918121338\n",
      "2.1212576230367026\n",
      "2.7497126261393228\n",
      "2.190617243448893\n",
      "2.0932693481445312\n",
      "2.7326784133911133\n",
      "2.161336580912272\n",
      "2.066011428833008\n",
      "2.715431531270345\n",
      "2.1327017148335776\n",
      "2.039372444152832\n",
      "2.697900136311849\n",
      "2.104604880015055\n",
      "2.0132428805033364\n",
      "2.6800123850504556\n",
      "2.0769413312276206\n",
      "1.9875179926554363\n",
      "2.6616957982381186\n",
      "2.0496114095052085\n",
      "1.962096373240153\n",
      "2.642878532409668\n",
      "2.02252197265625\n",
      "1.936881383260091\n",
      "2.6234896977742515\n",
      "1.9955841700236003\n",
      "1.9117813110351562\n",
      "2.6034579277038574\n",
      "1.9687166213989258\n",
      "1.88671080271403\n",
      "2.5827163060506186\n",
      "1.9418446222941081\n",
      "1.8615907033284504\n",
      "2.561199347178141\n",
      "1.9149006207784016\n",
      "1.8363491694132488\n",
      "2.538848559061686\n",
      "1.887824535369873\n",
      "1.810922622680664\n",
      "2.515610376993815\n",
      "1.860565185546875\n",
      "1.7852568626403809\n",
      "2.491440773010254\n",
      "1.8330802917480469\n",
      "1.7593073844909668\n",
      "2.4663073221842446\n",
      "1.8053375879923503\n",
      "1.7330420811971028\n",
      "2.440188725789388\n",
      "1.777316729227702\n",
      "1.7064398129781086\n",
      "2.4130800565083823\n",
      "1.749008337656657\n",
      "1.6794934272766113\n",
      "2.3849929173787436\n",
      "1.7204167048136394\n",
      "1.6522083282470703\n",
      "2.3559563954671225\n",
      "1.6915594736735027\n",
      "1.6246039072672527\n",
      "2.3260199228922525\n",
      "1.6624663670857747\n",
      "1.596712589263916\n",
      "2.2952518463134766\n",
      "1.6331812540690105\n",
      "1.5685780843098958\n",
      "2.2637402216593423\n",
      "1.6037583351135254\n",
      "1.5402549107869465\n",
      "2.231589953104655\n",
      "1.5742624600728352\n",
      "1.511805534362793\n",
      "2.1989213625590005\n",
      "1.5447648366292317\n",
      "1.4832984606424968\n",
      "2.1658665339152017\n",
      "1.5153419176737468\n",
      "1.454805056254069\n",
      "2.1325645446777344\n",
      "1.4860706329345703\n",
      "1.4263976414998372\n",
      "2.099157969156901\n",
      "1.4570274353027344\n",
      "1.3981456756591797\n",
      "2.0657876332600913\n",
      "1.4282838503519695\n",
      "1.370114803314209\n",
      "2.032588164011637\n",
      "1.3999052047729492\n",
      "1.3423633575439453\n",
      "1.9996849695841472\n",
      "1.3719480832417805\n",
      "1.3149419625600178\n",
      "1.9671905835469563\n",
      "1.3444595336914062\n",
      "1.2878923416137695\n",
      "1.9352030754089355\n",
      "1.3174763520558674\n",
      "1.2612468401590984\n",
      "1.9038039843241374\n",
      "1.2910254796346028\n",
      "1.2350289821624756\n",
      "1.8730603853861492\n",
      "1.26512344678243\n",
      "1.2092533906300862\n",
      "1.8430220286051433\n",
      "1.2397786776224773\n",
      "1.183927059173584\n",
      "1.8137253125508626\n",
      "1.2149910926818848\n",
      "1.1590509414672852\n",
      "1.7851924896240234\n",
      "1.1907541751861572\n",
      "1.1346200307210286\n",
      "1.7574351628621419\n",
      "1.1670561631520588\n",
      "1.1106255849202473\n",
      "1.7304541269938152\n",
      "1.1438811620076497\n",
      "1.0870561599731445\n",
      "1.7042417526245117\n",
      "1.1212104956309001\n",
      "1.0638981660207112\n",
      "1.678784688313802\n",
      "1.0990233421325684\n",
      "1.0411370595296223\n",
      "1.6540633837382\n",
      "1.0772984822591145\n",
      "1.0187593301137288\n",
      "1.6300544738769531\n",
      "1.056013584136963\n",
      "0.9967509905497233\n",
      "1.6067307790120442\n",
      "1.0351474285125732\n",
      "0.9751001993815104\n",
      "1.5840638478597004\n",
      "1.0146801471710205\n",
      "0.9537957509358724\n",
      "1.5620228449503581\n",
      "0.994592030843099\n",
      "0.9328294595082601\n",
      "1.5405766169230144\n",
      "0.9748660723368326\n",
      "0.9121939341227213\n",
      "1.5196925799051921\n",
      "0.9554861386617025\n",
      "0.8918844064076742\n",
      "1.499338944753011\n",
      "0.9364380836486816\n",
      "0.8718976974487305\n",
      "1.4794832865397136\n",
      "0.9177099863688151\n",
      "0.8522314230600992\n",
      "1.4600941340128581\n",
      "0.8992907206217448\n",
      "0.8328848679860433\n",
      "1.4411408106486003\n",
      "0.8811715443929037\n",
      "0.813857634862264\n",
      "1.4225924809773762\n",
      "0.8633447488149008\n",
      "0.7951499621073405\n",
      "1.4044206937154133\n",
      "0.8458037376403809\n",
      "0.7767621676127116\n",
      "1.3865981101989746\n",
      "0.8285435835520426\n",
      "0.7586936950683594\n",
      "1.3690975507100422\n",
      "0.8115594387054443\n",
      "0.7409447034200033\n",
      "1.3518945376078289\n",
      "0.7948481241861979\n",
      "0.7235145568847656\n",
      "1.3349655469258626\n",
      "0.7784063816070557\n",
      "0.7064021428426107\n",
      "1.318288803100586\n",
      "0.7622319857279459\n",
      "0.6896061897277832\n",
      "1.3018439610799153\n",
      "0.746323029200236\n",
      "0.6731252670288086\n",
      "1.285612424214681\n",
      "0.7306779225667318\n",
      "0.6569580634435018\n",
      "1.2695771058400471\n",
      "0.7152953147888184\n",
      "0.6411025126775106\n",
      "1.2537227471669514\n",
      "0.7001746495564779\n",
      "0.625557541847229\n",
      "1.2380354404449463\n",
      "0.6853143374125162\n",
      "0.6103216012318929\n",
      "1.2225023905436199\n",
      "0.6707142988840739\n",
      "0.595393697420756\n",
      "1.2071125507354736\n",
      "0.6563740571339926\n",
      "0.5807722409566244\n",
      "1.1918567021687825\n",
      "0.642292857170105\n",
      "0.5664572715759277\n",
      "1.17672594388326\n",
      "0.6284706592559814\n",
      "0.5524478356043497\n",
      "1.1617132027943928\n",
      "0.6149072249730428\n",
      "0.5387430588404337\n",
      "1.1468124389648438\n",
      "0.6016022761662801\n",
      "0.52534286181132\n",
      "1.1320186456044514\n",
      "0.588555653889974\n",
      "0.5122469266255697\n",
      "1.1173279285430908\n",
      "0.5757671991984049\n",
      "0.49945441881815594\n",
      "1.1027371088663738\n",
      "0.5632362763086954\n",
      "0.4869649410247803\n",
      "1.0882442792256672\n",
      "0.5509630044301351\n",
      "0.474777619043986\n",
      "1.0738476117451985\n",
      "0.5389469067255656\n",
      "0.4628913799921672\n",
      "1.059546709060669\n",
      "0.5271874666213989\n",
      "0.4513050715128581\n",
      "1.0453414122263591\n",
      "0.5156842072804769\n",
      "0.4400172233581543\n",
      "1.0312323570251465\n"
     ]
    }
   ],
   "source": [
    "rnn = LSTMNet(30,5,30)\n",
    "learning_rate = 0.05\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "inp_list = [[(1,2,3),(2,3,4)],[(2,3,4),(3,4,5)],[(5,6,7),(6,7,8)]]\n",
    "for i in range(100):\n",
    "    for inp,out in inp_list:\n",
    "        h_t,c_t = rnn.init_hidden_cell_state()\n",
    "        rnn.zero_grad()\n",
    "        total_loss = 0\n",
    "        for i,o in zip(inp,out):\n",
    "            #print(i,o)\n",
    "            inp_ten = get_input_tensor(i)\n",
    "            o_t,h_t, c_t = rnn(inp_ten,h_t,c_t)\n",
    "            category_tensor = torch.tensor([o], dtype=torch.long)\n",
    "            loss = criterion(o_t,category_tensor)\n",
    "            total_loss += loss\n",
    "        print(total_loss.item()/3)\n",
    "        total_loss.backward()\n",
    "        for p in rnn.parameters():\n",
    "            p.data.add_(p.grad.data, alpha=-learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
