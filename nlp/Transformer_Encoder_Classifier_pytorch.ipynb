{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils.py\n",
    "\n",
    "import torch\n",
    "from torchtext import data\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "class Dataset(object):\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.train_iterator = None\n",
    "        self.test_iterator = None\n",
    "        self.val_iterator = None\n",
    "        self.vocab = []\n",
    "        self.word_embeddings = {}\n",
    "    \n",
    "    def parse_label(self, label):\n",
    "        '''\n",
    "        Get the actual labels from label string\n",
    "        Input:\n",
    "            label (string) : labels of the form '__label__2'\n",
    "        Returns:\n",
    "            label (int) : integer value corresponding to label string\n",
    "        '''\n",
    "        return int(label.strip()[1:-1])\n",
    "\n",
    "    def get_pandas_df(self, filename):\n",
    "        '''\n",
    "        Load the data into Pandas.DataFrame object\n",
    "        This will be used to convert data to torchtext object\n",
    "        '''\n",
    "        print(filename)\n",
    "        with open(filename, 'r') as datafile:     \n",
    "            data = [line.strip().split(',', maxsplit=1) for line in datafile]\n",
    "            data_text = list(map(lambda x: x[1], data))\n",
    "            data_label = list(map(lambda x: self.parse_label(x[0]), data))\n",
    "\n",
    "        full_df = pd.DataFrame({\"text\":data_text, \"label\":data_label})\n",
    "        return full_df\n",
    "    \n",
    "    def load_data(self, train_file, test_file=None, val_file=None):\n",
    "        '''\n",
    "        Loads the data from files\n",
    "        Sets up iterators for training, validation and test data\n",
    "        Also create vocabulary and word embeddings based on the data\n",
    "        \n",
    "        Inputs:\n",
    "            train_file (String): path to training file\n",
    "            test_file (String): path to test file\n",
    "            val_file (String): path to validation file\n",
    "        '''\n",
    "\n",
    "        NLP = spacy.load('en')\n",
    "        tokenizer = lambda sent: [x.text for x in NLP.tokenizer(sent) if x.text != \" \"]\n",
    "        \n",
    "        # Creating Field for data\n",
    "        TEXT = data.Field(sequential=True, tokenize=tokenizer, lower=True, fix_length=self.config.max_sen_len)\n",
    "        LABEL = data.Field(sequential=False, use_vocab=False)\n",
    "        datafields = [(\"text\",TEXT),(\"label\",LABEL)]\n",
    "        \n",
    "        # Load data from pd.DataFrame into torchtext.data.Dataset\n",
    "        train_df = self.get_pandas_df(train_file)\n",
    "        train_examples = [data.Example.fromlist(i, datafields) for i in train_df.values.tolist()]\n",
    "        train_data = data.Dataset(train_examples, datafields)\n",
    "        \n",
    "        test_df = self.get_pandas_df(test_file)\n",
    "        test_examples = [data.Example.fromlist(i, datafields) for i in test_df.values.tolist()]\n",
    "        test_data = data.Dataset(test_examples, datafields)\n",
    "        \n",
    "        # If validation file exists, load it. Otherwise get validation data from training data\n",
    "        if val_file:\n",
    "            val_df = self.get_pandas_df(val_file)\n",
    "            val_examples = [data.Example.fromlist(i, datafields) for i in val_df.values.tolist()]\n",
    "            val_data = data.Dataset(val_examples, datafields)\n",
    "        else:\n",
    "            train_data, val_data = train_data.split(split_ratio=0.8)\n",
    "        \n",
    "        TEXT.build_vocab(train_data)\n",
    "        self.vocab = TEXT.vocab\n",
    "        \n",
    "        self.train_iterator = data.BucketIterator(\n",
    "            (train_data),\n",
    "            batch_size=self.config.batch_size,\n",
    "            sort_key=lambda x: len(x.text),\n",
    "            repeat=False,\n",
    "            shuffle=True)\n",
    "        \n",
    "        self.val_iterator, self.test_iterator = data.BucketIterator.splits(\n",
    "            (val_data, test_data),\n",
    "            batch_size=self.config.batch_size,\n",
    "            sort_key=lambda x: len(x.text),\n",
    "            repeat=False,\n",
    "            shuffle=False)\n",
    "        \n",
    "        print (\"Loaded {} training examples\".format(len(train_data)))\n",
    "        print (\"Loaded {} test examples\".format(len(test_data)))\n",
    "        print (\"Loaded {} validation examples\".format(len(val_data)))\n",
    "\n",
    "def evaluate_model(model, iterator):\n",
    "    all_preds = []\n",
    "    all_y = []\n",
    "    for idx,batch in enumerate(iterator):\n",
    "        if torch.cuda.is_available():\n",
    "            x = batch.text.cuda()\n",
    "        else:\n",
    "            x = batch.text\n",
    "        y_pred = model(x)\n",
    "        predicted = torch.max(y_pred.cpu().data, 1)[1] + 1\n",
    "        all_preds.extend(predicted.numpy())\n",
    "        all_y.extend(batch.label.numpy())\n",
    "    score = accuracy_score(all_y, np.array(all_preds).flatten())\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_utils.py\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import copy\n",
    "import math\n",
    "\n",
    "def clones(module, N):\n",
    "    \"Produce N identical layers.\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
    "\n",
    "class Embeddings(nn.Module):\n",
    "    '''\n",
    "    Usual Embedding layer with weights multiplied by sqrt(d_model)\n",
    "    '''\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.lut = nn.Embedding(vocab, d_model)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lut(x) * math.sqrt(self.d_model)\n",
    "    \n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"Implement the PE function.\"\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() *\n",
    "                             -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(torch.as_tensor(position.numpy() * div_term.unsqueeze(0).numpy()))\n",
    "        pe[:, 1::2] = torch.cos(torch.as_tensor(position.numpy() * div_term.unsqueeze(0).numpy()))#torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + Variable(self.pe[:, :x.size(1)], \n",
    "                         requires_grad=False)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sublayer.py\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    \"Construct a layer normalization module.\"\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n",
    "    \n",
    "class SublayerOutput(nn.Module):\n",
    "    '''\n",
    "    A residual connection followed by a layer norm.\n",
    "    '''\n",
    "    def __init__(self, size, dropout):\n",
    "        super(SublayerOutput, self).__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        \"Apply residual connection to any sublayer with the same size.\"\n",
    "        return x + self.dropout(sublayer(self.norm(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attention.py\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "#from train_utils import clones\n",
    "\n",
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "    \"Implementation of Scaled dot product attention\"\n",
    "    d_k = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    p_attn = F.softmax(scores, dim = -1)\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "    return torch.matmul(p_attn, value), p_attn\n",
    "\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        \"Take in model size and number of heads.\"\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        assert d_model % h == 0\n",
    "        # We assume d_v always equals d_k\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"Implements Multi-head attention\"\n",
    "        if mask is not None:\n",
    "            # Same mask applied to all h heads.\n",
    "            mask = mask.unsqueeze(1)\n",
    "        nbatches = query.size(0)\n",
    "        \n",
    "        # 1) Do all the linear projections in batch from d_model => h x d_k \n",
    "        query, key, value = \\\n",
    "            [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "             for l, x in zip(self.linears, (query, key, value))]\n",
    "        \n",
    "        # 2) Apply attention on all the projected vectors in batch. \n",
    "        x, self.attn = attention(query, key, value, mask=mask, \n",
    "                                 dropout=self.dropout)\n",
    "        \n",
    "        # 3) \"Concat\" using a view and apply a final linear. \n",
    "        x = x.transpose(1, 2).contiguous() \\\n",
    "             .view(nbatches, -1, self.h * self.d_k)\n",
    "        return self.linears[-1](x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder.py\n",
    "\n",
    "from torch import nn\n",
    "#from train_utils import clones\n",
    "#from sublayer import LayerNorm, SublayerOutput\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    '''\n",
    "    Transformer Encoder\n",
    "    \n",
    "    It is a stack of N layers.\n",
    "    '''\n",
    "    def __init__(self, layer, N):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)\n",
    "    \n",
    "class EncoderLayer(nn.Module):\n",
    "    '''\n",
    "    An encoder layer\n",
    "    \n",
    "    Made up of self-attention and a feed forward layer.\n",
    "    Each of these sublayers have residual and layer norm, implemented by SublayerOutput.\n",
    "    '''\n",
    "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer_output = clones(SublayerOutput(size, dropout), 2)\n",
    "        self.size = size\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        \"Transformer Encoder\"\n",
    "        x = self.sublayer_output[0](x, lambda x: self.self_attn(x, x, x, mask)) # Encoder self-attention\n",
    "        return self.sublayer_output[1](x, self.feed_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feed_forward.py\n",
    "\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"Positionwise feed-forward network.\"\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"Implements FFN equation.\"\n",
    "        return self.w_2(self.dropout(F.relu(self.w_1(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from copy import deepcopy\n",
    "#from train_utils import Embeddings,PositionalEncoding\n",
    "#from attention import MultiHeadedAttention\n",
    "#from encoder import EncoderLayer, Encoder\n",
    "#from feed_forward import PositionwiseFeedForward\n",
    "import numpy as np\n",
    "#from utils import *\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, config, src_vocab):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        h, N, dropout = self.config.h, self.config.N, self.config.dropout\n",
    "        d_model, d_ff = self.config.d_model, self.config.d_ff\n",
    "        \n",
    "        attn = MultiHeadedAttention(h, d_model)\n",
    "        ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "        position = PositionalEncoding(d_model, dropout)\n",
    "        \n",
    "        self.encoder = Encoder(EncoderLayer(config.d_model, deepcopy(attn), deepcopy(ff), dropout), N)\n",
    "        self.src_embed = nn.Sequential(Embeddings(config.d_model, src_vocab), deepcopy(position)) #Embeddings followed by PE\n",
    "\n",
    "        # Fully-Connected Layer\n",
    "        self.fc = nn.Linear(\n",
    "            self.config.d_model,\n",
    "            self.config.output_size\n",
    "        )\n",
    "        \n",
    "        # Softmax non-linearity\n",
    "        self.softmax = nn.Softmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded_sents = self.src_embed(x.permute(1,0)) # shape = (batch_size, sen_len, d_model)\n",
    "        encoded_sents = self.encoder(embedded_sents)\n",
    "        \n",
    "        # Convert input to (batch_size, d_model) for linear layer\n",
    "        final_feature_map = encoded_sents[:,-1,:]\n",
    "        final_out = self.fc(final_feature_map)\n",
    "        return self.softmax(final_out)\n",
    "    \n",
    "    def add_optimizer(self, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "    def add_loss_op(self, loss_op):\n",
    "        self.loss_op = loss_op\n",
    "    \n",
    "    def reduce_lr(self):\n",
    "        print(\"Reducing LR\")\n",
    "        for g in self.optimizer.param_groups:\n",
    "            g['lr'] = g['lr'] / 2\n",
    "                \n",
    "    def run_epoch(self, train_iterator, val_iterator, epoch):\n",
    "        train_losses = []\n",
    "        val_accuracies = []\n",
    "        losses = []\n",
    "        \n",
    "        # Reduce learning rate as number of epochs increase\n",
    "        if (epoch == int(self.config.max_epochs/3)) or (epoch == int(2*self.config.max_epochs/3)):\n",
    "            self.reduce_lr()\n",
    "            \n",
    "        for i, batch in enumerate(train_iterator):\n",
    "            self.optimizer.zero_grad()\n",
    "            if torch.cuda.is_available():\n",
    "                x = batch.text.cuda()\n",
    "                y = (batch.label - 1).type(torch.cuda.LongTensor)\n",
    "            else:\n",
    "                x = batch.text\n",
    "                y = (batch.label - 1).type(torch.LongTensor)\n",
    "            y_pred = self.__call__(x)\n",
    "            loss = self.loss_op(y_pred, y)\n",
    "            loss.backward()\n",
    "            losses.append(loss.data.cpu().numpy())\n",
    "            self.optimizer.step()\n",
    "    \n",
    "            if i % 100 == 0:\n",
    "                print(\"Iter: {}\".format(i+1))\n",
    "                avg_train_loss = np.mean(losses)\n",
    "                train_losses.append(avg_train_loss)\n",
    "                print(\"\\tAverage training loss: {:.5f}\".format(avg_train_loss))\n",
    "                losses = []\n",
    "                \n",
    "                # Evalute Accuracy on validation set\n",
    "                val_accuracy = evaluate_model(self, val_iterator)\n",
    "                print(\"\\tVal Accuracy: {:.4f}\".format(val_accuracy))\n",
    "                self.train()\n",
    "                \n",
    "        return train_losses, val_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    N = 1 #6 in Transformer Paper\n",
    "    d_model = 256 #512 in Transformer Paper\n",
    "    d_ff = 512 #2048 in Transformer Paper\n",
    "    h = 8\n",
    "    dropout = 0.1\n",
    "    output_size = 4\n",
    "    lr = 0.0003\n",
    "    max_epochs = 35\n",
    "    batch_size = 128\n",
    "    max_sen_len = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.csv\n",
      "test.csv\n",
      "Loaded 96000 training examples\n",
      "Loaded 7600 test examples\n",
      "Loaded 24000 validation examples\n",
      "Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/tljh/user/lib/python3.6/site-packages/ipykernel_launcher.py:44: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 1\n",
      "\tAverage training loss: -0.24565\n",
      "\tVal Accuracy: 0.2527\n",
      "Iter: 101\n",
      "\tAverage training loss: -0.25069\n",
      "\tVal Accuracy: 0.2531\n",
      "Iter: 201\n",
      "\tAverage training loss: -0.25736\n",
      "\tVal Accuracy: 0.2833\n",
      "Iter: 301\n",
      "\tAverage training loss: -0.31337\n",
      "\tVal Accuracy: 0.3795\n",
      "Iter: 401\n",
      "\tAverage training loss: -0.40257\n",
      "\tVal Accuracy: 0.4303\n",
      "Iter: 501\n",
      "\tAverage training loss: -0.44901\n",
      "\tVal Accuracy: 0.4523\n",
      "Iter: 601\n",
      "\tAverage training loss: -0.47402\n",
      "\tVal Accuracy: 0.4945\n",
      "Iter: 701\n",
      "\tAverage training loss: -0.50048\n",
      "\tVal Accuracy: 0.5183\n",
      "Epoch: 1\n",
      "Iter: 1\n",
      "\tAverage training loss: -0.53797\n",
      "\tVal Accuracy: 0.5256\n",
      "Iter: 101\n",
      "\tAverage training loss: -0.53654\n",
      "\tVal Accuracy: 0.5509\n",
      "Iter: 201\n",
      "\tAverage training loss: -0.55716\n",
      "\tVal Accuracy: 0.5635\n",
      "Iter: 301\n",
      "\tAverage training loss: -0.58513\n",
      "\tVal Accuracy: 0.6039\n",
      "Iter: 401\n",
      "\tAverage training loss: -0.61813\n",
      "\tVal Accuracy: 0.6347\n",
      "Iter: 501\n",
      "\tAverage training loss: -0.65015\n",
      "\tVal Accuracy: 0.6694\n",
      "Iter: 601\n",
      "\tAverage training loss: -0.68392\n",
      "\tVal Accuracy: 0.6994\n",
      "Iter: 701\n",
      "\tAverage training loss: -0.68934\n",
      "\tVal Accuracy: 0.7020\n",
      "Epoch: 2\n",
      "Iter: 1\n",
      "\tAverage training loss: -0.74183\n",
      "\tVal Accuracy: 0.7166\n",
      "Iter: 101\n",
      "\tAverage training loss: -0.72470\n",
      "\tVal Accuracy: 0.7265\n",
      "Iter: 201\n",
      "\tAverage training loss: -0.73348\n",
      "\tVal Accuracy: 0.7356\n",
      "Iter: 301\n",
      "\tAverage training loss: -0.74432\n",
      "\tVal Accuracy: 0.7441\n",
      "Iter: 401\n",
      "\tAverage training loss: -0.74466\n",
      "\tVal Accuracy: 0.7496\n",
      "Iter: 501\n",
      "\tAverage training loss: -0.75901\n",
      "\tVal Accuracy: 0.7544\n",
      "Iter: 601\n",
      "\tAverage training loss: -0.75688\n",
      "\tVal Accuracy: 0.7646\n",
      "Iter: 701\n",
      "\tAverage training loss: -0.77307\n",
      "\tVal Accuracy: 0.7680\n",
      "Epoch: 3\n",
      "Iter: 1\n",
      "\tAverage training loss: -0.74557\n",
      "\tVal Accuracy: 0.7653\n",
      "Iter: 101\n",
      "\tAverage training loss: -0.77925\n",
      "\tVal Accuracy: 0.7726\n",
      "Iter: 201\n",
      "\tAverage training loss: -0.79161\n",
      "\tVal Accuracy: 0.7798\n",
      "Iter: 301\n",
      "\tAverage training loss: -0.79248\n",
      "\tVal Accuracy: 0.7880\n",
      "Iter: 401\n",
      "\tAverage training loss: -0.79189\n",
      "\tVal Accuracy: 0.7906\n",
      "Iter: 501\n",
      "\tAverage training loss: -0.78912\n",
      "\tVal Accuracy: 0.7884\n",
      "Iter: 601\n",
      "\tAverage training loss: -0.79783\n",
      "\tVal Accuracy: 0.7912\n",
      "Iter: 701\n",
      "\tAverage training loss: -0.79766\n",
      "\tVal Accuracy: 0.7946\n",
      "Epoch: 4\n",
      "Iter: 1\n",
      "\tAverage training loss: -0.78163\n",
      "\tVal Accuracy: 0.7977\n",
      "Iter: 101\n",
      "\tAverage training loss: -0.80944\n",
      "\tVal Accuracy: 0.8025\n",
      "Iter: 201\n",
      "\tAverage training loss: -0.81160\n",
      "\tVal Accuracy: 0.8052\n",
      "Iter: 301\n",
      "\tAverage training loss: -0.82065\n",
      "\tVal Accuracy: 0.8057\n",
      "Iter: 401\n",
      "\tAverage training loss: -0.80907\n",
      "\tVal Accuracy: 0.8084\n",
      "Iter: 501\n",
      "\tAverage training loss: -0.81658\n",
      "\tVal Accuracy: 0.8138\n",
      "Iter: 601\n",
      "\tAverage training loss: -0.82380\n",
      "\tVal Accuracy: 0.8185\n",
      "Iter: 701\n",
      "\tAverage training loss: -0.82146\n",
      "\tVal Accuracy: 0.8131\n",
      "Epoch: 5\n",
      "Iter: 1\n",
      "\tAverage training loss: -0.85085\n",
      "\tVal Accuracy: 0.8124\n",
      "Iter: 101\n",
      "\tAverage training loss: -0.83475\n",
      "\tVal Accuracy: 0.8156\n",
      "Iter: 201\n",
      "\tAverage training loss: -0.83030\n",
      "\tVal Accuracy: 0.8177\n",
      "Iter: 301\n",
      "\tAverage training loss: -0.83155\n",
      "\tVal Accuracy: 0.8135\n",
      "Iter: 401\n",
      "\tAverage training loss: -0.82748\n",
      "\tVal Accuracy: 0.8123\n",
      "Iter: 501\n",
      "\tAverage training loss: -0.83076\n",
      "\tVal Accuracy: 0.8172\n",
      "Iter: 601\n",
      "\tAverage training loss: -0.83282\n",
      "\tVal Accuracy: 0.8202\n",
      "Iter: 701\n",
      "\tAverage training loss: -0.83551\n",
      "\tVal Accuracy: 0.8243\n",
      "Epoch: 6\n",
      "Iter: 1\n",
      "\tAverage training loss: -0.83449\n",
      "\tVal Accuracy: 0.8263\n",
      "Iter: 101\n",
      "\tAverage training loss: -0.84154\n",
      "\tVal Accuracy: 0.8271\n",
      "Iter: 201\n",
      "\tAverage training loss: -0.84470\n",
      "\tVal Accuracy: 0.8257\n",
      "Iter: 301\n",
      "\tAverage training loss: -0.84197\n",
      "\tVal Accuracy: 0.8241\n",
      "Iter: 401\n",
      "\tAverage training loss: -0.84065\n",
      "\tVal Accuracy: 0.8329\n",
      "Iter: 501\n",
      "\tAverage training loss: -0.84051\n",
      "\tVal Accuracy: 0.8305\n",
      "Iter: 601\n",
      "\tAverage training loss: -0.84901\n",
      "\tVal Accuracy: 0.8241\n",
      "Iter: 701\n",
      "\tAverage training loss: -0.84584\n",
      "\tVal Accuracy: 0.8253\n",
      "Epoch: 7\n",
      "Iter: 1\n",
      "\tAverage training loss: -0.82207\n",
      "\tVal Accuracy: 0.8270\n",
      "Iter: 101\n",
      "\tAverage training loss: -0.84845\n",
      "\tVal Accuracy: 0.8335\n",
      "Iter: 201\n",
      "\tAverage training loss: -0.84704\n",
      "\tVal Accuracy: 0.8287\n",
      "Iter: 301\n",
      "\tAverage training loss: -0.85485\n",
      "\tVal Accuracy: 0.8391\n",
      "Iter: 401\n",
      "\tAverage training loss: -0.85412\n",
      "\tVal Accuracy: 0.8410\n",
      "Iter: 501\n",
      "\tAverage training loss: -0.84987\n",
      "\tVal Accuracy: 0.8375\n",
      "Iter: 601\n",
      "\tAverage training loss: -0.84892\n",
      "\tVal Accuracy: 0.8355\n",
      "Iter: 701\n",
      "\tAverage training loss: -0.85434\n",
      "\tVal Accuracy: 0.8414\n",
      "Epoch: 8\n",
      "Iter: 1\n",
      "\tAverage training loss: -0.85156\n",
      "\tVal Accuracy: 0.8407\n",
      "Iter: 101\n",
      "\tAverage training loss: -0.85918\n",
      "\tVal Accuracy: 0.8390\n",
      "Iter: 201\n",
      "\tAverage training loss: -0.85753\n",
      "\tVal Accuracy: 0.8375\n",
      "Iter: 301\n",
      "\tAverage training loss: -0.86304\n",
      "\tVal Accuracy: 0.8431\n",
      "Iter: 401\n",
      "\tAverage training loss: -0.85903\n",
      "\tVal Accuracy: 0.8437\n",
      "Iter: 501\n",
      "\tAverage training loss: -0.86055\n",
      "\tVal Accuracy: 0.8446\n",
      "Iter: 601\n",
      "\tAverage training loss: -0.85882\n",
      "\tVal Accuracy: 0.8433\n",
      "Iter: 701\n",
      "\tAverage training loss: -0.85569\n",
      "\tVal Accuracy: 0.8414\n",
      "Epoch: 9\n",
      "Iter: 1\n",
      "\tAverage training loss: -0.84067\n",
      "\tVal Accuracy: 0.8485\n",
      "Iter: 101\n",
      "\tAverage training loss: -0.86714\n",
      "\tVal Accuracy: 0.8484\n",
      "Iter: 201\n",
      "\tAverage training loss: -0.86713\n",
      "\tVal Accuracy: 0.8468\n",
      "Iter: 301\n",
      "\tAverage training loss: -0.86872\n",
      "\tVal Accuracy: 0.8418\n",
      "Iter: 401\n",
      "\tAverage training loss: -0.86819\n",
      "\tVal Accuracy: 0.8490\n",
      "Iter: 501\n",
      "\tAverage training loss: -0.86220\n",
      "\tVal Accuracy: 0.8524\n",
      "Iter: 601\n",
      "\tAverage training loss: -0.86625\n",
      "\tVal Accuracy: 0.8472\n",
      "Iter: 701\n",
      "\tAverage training loss: -0.86181\n",
      "\tVal Accuracy: 0.8517\n",
      "Epoch: 10\n",
      "Iter: 1\n",
      "\tAverage training loss: -0.90319\n",
      "\tVal Accuracy: 0.8510\n",
      "Iter: 101\n",
      "\tAverage training loss: -0.87177\n",
      "\tVal Accuracy: 0.8501\n",
      "Iter: 201\n",
      "\tAverage training loss: -0.87257\n",
      "\tVal Accuracy: 0.8498\n",
      "Iter: 301\n",
      "\tAverage training loss: -0.86521\n",
      "\tVal Accuracy: 0.8496\n",
      "Iter: 401\n",
      "\tAverage training loss: -0.86813\n",
      "\tVal Accuracy: 0.8502\n",
      "Iter: 501\n",
      "\tAverage training loss: -0.86545\n",
      "\tVal Accuracy: 0.8458\n",
      "Iter: 601\n",
      "\tAverage training loss: -0.86898\n",
      "\tVal Accuracy: 0.8459\n",
      "Iter: 701\n",
      "\tAverage training loss: -0.86942\n",
      "\tVal Accuracy: 0.8473\n",
      "Epoch: 11\n",
      "Reducing LR\n",
      "Iter: 1\n",
      "\tAverage training loss: -0.90319\n",
      "\tVal Accuracy: 0.8545\n",
      "Iter: 101\n",
      "\tAverage training loss: -0.87609\n",
      "\tVal Accuracy: 0.8570\n",
      "Iter: 201\n",
      "\tAverage training loss: -0.87247\n",
      "\tVal Accuracy: 0.8580\n",
      "Iter: 301\n",
      "\tAverage training loss: -0.87575\n",
      "\tVal Accuracy: 0.8565\n",
      "Iter: 401\n",
      "\tAverage training loss: -0.87804\n",
      "\tVal Accuracy: 0.8576\n",
      "Iter: 501\n",
      "\tAverage training loss: -0.87626\n",
      "\tVal Accuracy: 0.8580\n",
      "Iter: 601\n",
      "\tAverage training loss: -0.87896\n",
      "\tVal Accuracy: 0.8593\n",
      "Iter: 701\n",
      "\tAverage training loss: -0.88035\n",
      "\tVal Accuracy: 0.8609\n",
      "Epoch: 12\n",
      "Iter: 1\n",
      "\tAverage training loss: -0.88886\n",
      "\tVal Accuracy: 0.8580\n",
      "Iter: 101\n",
      "\tAverage training loss: -0.88112\n",
      "\tVal Accuracy: 0.8574\n",
      "Iter: 201\n",
      "\tAverage training loss: -0.88337\n",
      "\tVal Accuracy: 0.8629\n",
      "Iter: 301\n",
      "\tAverage training loss: -0.87772\n",
      "\tVal Accuracy: 0.8635\n",
      "Iter: 401\n",
      "\tAverage training loss: -0.88189\n",
      "\tVal Accuracy: 0.8600\n",
      "Iter: 501\n",
      "\tAverage training loss: -0.88352\n",
      "\tVal Accuracy: 0.8610\n",
      "Iter: 601\n",
      "\tAverage training loss: -0.88151\n",
      "\tVal Accuracy: 0.8607\n",
      "Iter: 701\n",
      "\tAverage training loss: -0.88003\n",
      "\tVal Accuracy: 0.8610\n",
      "Epoch: 13\n",
      "Iter: 1\n",
      "\tAverage training loss: -0.91105\n",
      "\tVal Accuracy: 0.8633\n",
      "Iter: 101\n",
      "\tAverage training loss: -0.87960\n",
      "\tVal Accuracy: 0.8637\n",
      "Iter: 201\n",
      "\tAverage training loss: -0.88496\n",
      "\tVal Accuracy: 0.8591\n",
      "Iter: 301\n",
      "\tAverage training loss: -0.88311\n",
      "\tVal Accuracy: 0.8636\n",
      "Iter: 401\n",
      "\tAverage training loss: -0.88702\n",
      "\tVal Accuracy: 0.8658\n",
      "Iter: 501\n",
      "\tAverage training loss: -0.88435\n",
      "\tVal Accuracy: 0.8597\n",
      "Iter: 601\n",
      "\tAverage training loss: -0.88261\n",
      "\tVal Accuracy: 0.8675\n",
      "Iter: 701\n",
      "\tAverage training loss: -0.88420\n",
      "\tVal Accuracy: 0.8620\n",
      "Epoch: 14\n",
      "Iter: 1\n",
      "\tAverage training loss: -0.89364\n",
      "\tVal Accuracy: 0.8626\n",
      "Iter: 101\n",
      "\tAverage training loss: -0.88623\n",
      "\tVal Accuracy: 0.8642\n",
      "Iter: 201\n",
      "\tAverage training loss: -0.88335\n",
      "\tVal Accuracy: 0.8643\n",
      "Iter: 301\n",
      "\tAverage training loss: -0.88579\n",
      "\tVal Accuracy: 0.8626\n",
      "Iter: 401\n",
      "\tAverage training loss: -0.88315\n",
      "\tVal Accuracy: 0.8580\n",
      "Iter: 501\n",
      "\tAverage training loss: -0.88564\n",
      "\tVal Accuracy: 0.8640\n",
      "Iter: 601\n",
      "\tAverage training loss: -0.88449\n",
      "\tVal Accuracy: 0.8635\n",
      "Iter: 701\n",
      "\tAverage training loss: -0.88788\n",
      "\tVal Accuracy: 0.8589\n",
      "Epoch: 15\n",
      "Iter: 1\n",
      "\tAverage training loss: -0.89166\n",
      "\tVal Accuracy: 0.8630\n",
      "Iter: 101\n",
      "\tAverage training loss: -0.88519\n",
      "\tVal Accuracy: 0.8635\n",
      "Iter: 201\n",
      "\tAverage training loss: -0.88971\n",
      "\tVal Accuracy: 0.8638\n",
      "Iter: 301\n",
      "\tAverage training loss: -0.88769\n",
      "\tVal Accuracy: 0.8655\n",
      "Iter: 401\n",
      "\tAverage training loss: -0.88584\n",
      "\tVal Accuracy: 0.8642\n",
      "Iter: 501\n",
      "\tAverage training loss: -0.89130\n",
      "\tVal Accuracy: 0.8633\n",
      "Iter: 601\n",
      "\tAverage training loss: -0.88713\n",
      "\tVal Accuracy: 0.8672\n",
      "Iter: 701\n",
      "\tAverage training loss: -0.88859\n",
      "\tVal Accuracy: 0.8692\n",
      "Epoch: 16\n",
      "Iter: 1\n",
      "\tAverage training loss: -0.91964\n",
      "\tVal Accuracy: 0.8641\n",
      "Iter: 101\n",
      "\tAverage training loss: -0.88566\n",
      "\tVal Accuracy: 0.8680\n",
      "Iter: 201\n",
      "\tAverage training loss: -0.89129\n",
      "\tVal Accuracy: 0.8674\n",
      "Iter: 301\n",
      "\tAverage training loss: -0.88861\n",
      "\tVal Accuracy: 0.8670\n",
      "Iter: 401\n",
      "\tAverage training loss: -0.88916\n",
      "\tVal Accuracy: 0.8672\n",
      "Iter: 501\n",
      "\tAverage training loss: -0.88816\n",
      "\tVal Accuracy: 0.8630\n",
      "Iter: 601\n",
      "\tAverage training loss: -0.88977\n",
      "\tVal Accuracy: 0.8695\n",
      "Iter: 701\n",
      "\tAverage training loss: -0.88884\n",
      "\tVal Accuracy: 0.8677\n",
      "Epoch: 17\n",
      "Iter: 1\n",
      "\tAverage training loss: -0.93221\n",
      "\tVal Accuracy: 0.8679\n",
      "Iter: 101\n",
      "\tAverage training loss: -0.89520\n",
      "\tVal Accuracy: 0.8643\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-95-e8259d579106>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Epoch: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0mtrain_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mval_accuracies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_accuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-93-b8b63115f014>\u001b[0m in \u001b[0;36mrun_epoch\u001b[0;34m(self, train_iterator, val_iterator, epoch)\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train.py\n",
    "\n",
    "#from utils import *\n",
    "#from model import *\n",
    "#from config import Config\n",
    "import sys\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "#if __name__=='__main__':\n",
    "config = Config()\n",
    "train_file = 'train.csv'\n",
    "# if len(sys.argv) > 2:\n",
    "#     train_file = sys.argv[1]\n",
    "test_file = 'test.csv'\n",
    "# if len(sys.argv) > 3:\n",
    "#     test_file = sys.argv[2]\n",
    "\n",
    "dataset = Dataset(config)\n",
    "dataset.load_data(train_file, test_file)\n",
    "\n",
    "# Create Model with specified optimizer and loss function\n",
    "##############################################################\n",
    "model = Transformer(config, len(dataset.vocab))\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "model.train()\n",
    "optimizer = optim.Adam(model.parameters(), lr=config.lr)\n",
    "NLLLoss = nn.NLLLoss()\n",
    "model.add_optimizer(optimizer)\n",
    "model.add_loss_op(NLLLoss)\n",
    "##############################################################\n",
    "\n",
    "train_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "for i in range(config.max_epochs):\n",
    "    print (\"Epoch: {}\".format(i))\n",
    "    train_loss,val_accuracy = model.run_epoch(dataset.train_iterator, dataset.val_iterator, i)\n",
    "    train_losses.append(train_loss)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "\n",
    "train_acc = evaluate_model(model, dataset.train_iterator)\n",
    "val_acc = evaluate_model(model, dataset.val_iterator)\n",
    "test_acc = evaluate_model(model, dataset.test_iterator)\n",
    "\n",
    "print ('Final Training Accuracy: {:.4f}'.format(train_acc))\n",
    "print ('Final Validation Accuracy: {:.4f}'.format(val_acc))\n",
    "print ('Final Test Accuracy: {:.4f}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# csv_url_train = \"https://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/train.csv\"\n",
    "# csv_url_test = \"https://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/test.csv\"\n",
    "# req = requests.get(csv_url_train)\n",
    "# url_content = req.content\n",
    "# csv_file = open('train.csv', 'wb')\n",
    "\n",
    "# csv_file.write(url_content)\n",
    "# csv_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# req = requests.get(csv_url_test)\n",
    "# url_content = req.content\n",
    "# csv_file = open('test.csv', 'wb')\n",
    "\n",
    "# csv_file.write(url_content)\n",
    "# csv_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For decoding task\n",
    "class EncoderDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A standard Encoder-Decoder architecture. Base for this and many \n",
    "    other models.\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.generator = generator\n",
    "        \n",
    "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "        \"Take in and process masked src and target sequences.\"\n",
    "        return self.decode(self.encode(src, src_mask), src_mask,\n",
    "                            tgt, tgt_mask)\n",
    "    \n",
    "    def encode(self, src, src_mask):\n",
    "        return self.encoder(self.src_embed(src), src_mask)\n",
    "    \n",
    "    def decode(self, memory, src_mask, tgt, tgt_mask):\n",
    "        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"Generic N layer decoder with masking.\"\n",
    "    def __init__(self, layer, N):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "        \n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, memory, src_mask, tgt_mask)\n",
    "        return self.norm(x)\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    \"Decoder is made of self-attn, src-attn, and feed forward (defined below)\"\n",
    "    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.size = size\n",
    "        self.self_attn = self_attn\n",
    "        self.src_attn = src_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 3)\n",
    " \n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        \"Follow Figure 1 (right) for connections.\"\n",
    "        m = memory\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n",
    "        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))\n",
    "        return self.sublayer[2](x, self.feed_forward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
