{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "class Preprocessing:\n",
    "\t\n",
    "\tdef __init__(self, args):\n",
    "\t\tself.data = 'tweets.csv'\n",
    "\t\tself.max_len = args.max_len\n",
    "\t\tself.max_words = args.max_words\n",
    "\t\tself.test_size = args.test_size\n",
    "\t\t\n",
    "\tdef load_data(self):\n",
    "\t\tdf = pd.read_csv(self.data)\n",
    "\t\tdf.drop(['id','keyword','location'], axis=1, inplace=True)\n",
    "\t\t\n",
    "\t\tX = df['text'].values\n",
    "\t\tY = df['target'].values\n",
    "\t\t\n",
    "\t\tself.x_train, self.x_test, self.y_train, self.y_test = train_test_split(X, Y, test_size=self.test_size)\n",
    "\t\t\n",
    "\tdef prepare_tokens(self):\n",
    "\t\tself.tokens = Tokenizer(num_words=self.max_words)\n",
    "\t\tself.tokens.fit_on_texts(self.x_train)\n",
    "\n",
    "\tdef sequence_to_token(self, x):\n",
    "\t\tsequences = self.tokens.texts_to_sequences(x)\n",
    "\t\treturn sequence.pad_sequences(sequences, maxlen=self.max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TweetClassifier(nn.ModuleList):\n",
    "\n",
    "\tdef __init__(self, args):\n",
    "\t\tsuper(TweetClassifier, self).__init__()\n",
    "\t\t\n",
    "\t\tself.batch_size = args.batch_size\n",
    "\t\tself.hidden_dim = args.hidden_dim\n",
    "\t\tself.LSTM_layers = args.lstm_layers\n",
    "\t\tself.input_size = args.max_words # embedding dimention\n",
    "\t\t\n",
    "\t\tself.dropout = nn.Dropout(0.5)\n",
    "\t\tself.embedding = nn.Embedding(self.input_size, self.hidden_dim, padding_idx=0)\n",
    "\t\tself.lstm = nn.LSTM(input_size=self.hidden_dim, hidden_size=self.hidden_dim, num_layers=self.LSTM_layers, batch_first=True)\n",
    "\t\tself.fc1 = nn.Linear(in_features=self.hidden_dim, out_features=257)\n",
    "\t\tself.fc2 = nn.Linear(257, 1)\n",
    "\t\t\n",
    "\tdef forward(self, x):\n",
    "\t\n",
    "\t\th = torch.zeros((self.LSTM_layers, x.size(0), self.hidden_dim))\n",
    "\t\tc = torch.zeros((self.LSTM_layers, x.size(0), self.hidden_dim))\n",
    "\t\t\n",
    "\t\ttorch.nn.init.xavier_normal_(h)\n",
    "\t\ttorch.nn.init.xavier_normal_(c)\n",
    "\n",
    "\t\tout = self.embedding(x)\n",
    "\t\tout, (hidden, cell) = self.lstm(out, (h,c))\n",
    "\t\tout = self.dropout(out)\n",
    "\t\tout = torch.relu_(self.fc1(out[:,-1,:]))\n",
    "\t\tout = self.dropout(out)\n",
    "\t\tout = torch.sigmoid(self.fc2(out))\n",
    "\n",
    "\t\treturn out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#from src import parameter_parser\n",
    "\n",
    "\n",
    "class DatasetMaper(Dataset):\n",
    "\t'''\n",
    "\tHandles batches of dataset\n",
    "\t'''\n",
    "\tdef __init__(self, x, y):\n",
    "\t\tself.x = x\n",
    "\t\tself.y = y\n",
    "\t\t\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.x)\n",
    "\t\t\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\treturn self.x[idx], self.y[idx]\n",
    "\t\t\n",
    "\n",
    "class Execute:\n",
    "\t'''\n",
    "\tClass for execution. Initializes the preprocessing as well as the \n",
    "\tTweet Classifier model\n",
    "\t'''\n",
    "\n",
    "\tdef __init__(self, args):\n",
    "\t\tself.__init_data__(args)\n",
    "\t\t\n",
    "\t\tself.args = args\n",
    "\t\tself.batch_size = args.batch_size\n",
    "\t\t\n",
    "\t\tself.model = TweetClassifier(args)\n",
    "\t\t\n",
    "\tdef __init_data__(self, args):\n",
    "\t\t'''\n",
    "\t\tInitialize preprocessing from raw dataset to dataset split into training and testing\n",
    "\t\tTraining and test datasets are index strings that refer to tokens\n",
    "\t\t'''\n",
    "\t\tself.preprocessing = Preprocessing(args)\n",
    "\t\tself.preprocessing.load_data()\n",
    "\t\tself.preprocessing.prepare_tokens()\n",
    "\n",
    "\t\traw_x_train = self.preprocessing.x_train\n",
    "\t\traw_x_test = self.preprocessing.x_test\n",
    "\t\t\n",
    "\t\tself.y_train = self.preprocessing.y_train\n",
    "\t\tself.y_test = self.preprocessing.y_test\n",
    "\n",
    "\t\tself.x_train = self.preprocessing.sequence_to_token(raw_x_train)\n",
    "\t\tself.x_test = self.preprocessing.sequence_to_token(raw_x_test)\n",
    "\t\t\n",
    "\tdef train(self):\n",
    "\t\t\n",
    "\t\ttraining_set = DatasetMaper(self.x_train, self.y_train)\n",
    "\t\ttest_set = DatasetMaper(self.x_test, self.y_test)\n",
    "\t\t\n",
    "\t\tself.loader_training = DataLoader(training_set, batch_size=self.batch_size)\n",
    "\t\tself.loader_test = DataLoader(test_set)\n",
    "\t\t\n",
    "\t\toptimizer = optim.RMSprop(self.model.parameters(), lr=args.learning_rate)\n",
    "\t\tfor epoch in range(args.epochs):\n",
    "\t\t\t\n",
    "\t\t\tpredictions = []\n",
    "\t\t\t\n",
    "\t\t\tself.model.train()\n",
    "\t\t\t\n",
    "\t\t\tfor x_batch, y_batch in self.loader_training:\n",
    "\t\t\t\t\n",
    "\t\t\t\tx = x_batch.type(torch.LongTensor)\n",
    "\t\t\t\ty = y_batch.type(torch.FloatTensor)\n",
    "\t\t\t\t\n",
    "\t\t\t\ty_pred = self.model(x)\n",
    "\t\t\t\t\n",
    "\t\t\t\tloss = F.binary_cross_entropy(y_pred, y)\n",
    "\t\t\t\t\n",
    "\t\t\t\toptimizer.zero_grad()\n",
    "\t\t\t\t\n",
    "\t\t\t\tloss.backward()\n",
    "\t\t\t\t\n",
    "\t\t\t\toptimizer.step()\n",
    "\t\t\t\t\n",
    "\t\t\t\tpredictions += list(y_pred.squeeze().detach().numpy())\n",
    "\t\t\t\n",
    "\t\t\ttest_predictions = self.evaluation()\n",
    "\t\t\t\n",
    "\t\t\ttrain_accuary = self.calculate_accuray(self.y_train, predictions)\n",
    "\t\t\ttest_accuracy = self.calculate_accuray(self.y_test, test_predictions)\n",
    "\t\t\t\n",
    "\t\t\tprint(\"Epoch: %d, loss: %.5f, Train accuracy: %.5f, Test accuracy: %.5f\" % (epoch+1, loss.item(), train_accuary, test_accuracy))\n",
    "\t\t\t\n",
    "\tdef evaluation(self):\n",
    "\n",
    "\t\tpredictions = []\n",
    "\t\tself.model.eval()\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\tfor x_batch, y_batch in self.loader_test:\n",
    "\t\t\t\tx = x_batch.type(torch.LongTensor)\n",
    "\t\t\t\ty = y_batch.type(torch.FloatTensor)\n",
    "\t\t\t\t\n",
    "\t\t\t\ty_pred = self.model(x)\n",
    "\t\t\t\tpredictions += list(y_pred.detach().numpy())\n",
    "\t\t\t\t\n",
    "\t\treturn predictions\n",
    "\t\t\t\n",
    "\t@staticmethod\n",
    "\tdef calculate_accuray(grand_truth, predictions):\n",
    "\t\ttrue_positives = 0\n",
    "\t\ttrue_negatives = 0\n",
    "\t\t\n",
    "\t\tfor true, pred in zip(grand_truth, predictions):\n",
    "\t\t\tif (pred > 0.5) and (true == 1):\n",
    "\t\t\t\ttrue_positives += 1\n",
    "\t\t\telif (pred < 0.5) and (true == 0):\n",
    "\t\t\t\ttrue_negatives += 1\n",
    "\t\t\telse:\n",
    "\t\t\t\tpass\n",
    "\t\t\t\t\n",
    "\t\treturn (true_positives+true_negatives) / len(grand_truth)\n",
    "\t\n",
    "# if __name__ == \"__main__\":\n",
    "\t\n",
    "# \targs = parameter_parser()\n",
    "\t\n",
    "# \texecute = Execute(args)\n",
    "# \texecute.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class args:\n",
    "    epochs = 10\n",
    "    learning_rate = 0.01\n",
    "    hidden_dim = 128\n",
    "    lstm_layers = 2\n",
    "    batch_size = 64\n",
    "    test_size = 0.2\n",
    "    max_len = 20\n",
    "    max_words = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "execute = Execute(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/tljh/user/lib/python3.6/site-packages/ipykernel_launcher.py:87: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])) is deprecated. Please ensure they have the same size.\n",
      "/opt/tljh/user/lib/python3.6/site-packages/ipykernel_launcher.py:87: UserWarning: Using a target size (torch.Size([10])) that is different to the input size (torch.Size([10, 1])) is deprecated. Please ensure they have the same size.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, loss: 0.55636, Train accuracy: 0.55977, Test accuracy: 0.62968\n",
      "Epoch: 2, loss: 0.45177, Train accuracy: 0.63990, Test accuracy: 0.68418\n",
      "Epoch: 3, loss: 0.68899, Train accuracy: 0.75632, Test accuracy: 0.71963\n",
      "Epoch: 4, loss: 0.12717, Train accuracy: 0.81921, Test accuracy: 0.75968\n",
      "Epoch: 5, loss: 0.07534, Train accuracy: 0.85665, Test accuracy: 0.75181\n",
      "Epoch: 6, loss: 0.01073, Train accuracy: 0.88506, Test accuracy: 0.75181\n",
      "Epoch: 7, loss: 0.02390, Train accuracy: 0.91494, Test accuracy: 0.74655\n",
      "Epoch: 8, loss: 0.01437, Train accuracy: 0.92397, Test accuracy: 0.73605\n",
      "Epoch: 9, loss: 0.02795, Train accuracy: 0.92660, Test accuracy: 0.74327\n",
      "Epoch: 10, loss: 0.04867, Train accuracy: 0.93777, Test accuracy: 0.75181\n"
     ]
    }
   ],
   "source": [
    "execute.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#https://github.com/FernandoLpz/Text-Generation-BiLSTM-PyTorch\n",
    "#https://github.com/AnubhavGupta3377/Text-Classification-Models-Pytorch\n",
    "5//4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float(\"3\".strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
