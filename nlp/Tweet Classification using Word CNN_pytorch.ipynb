{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "class Preprocessing:\n",
    "\t\n",
    "\tdef __init__(self, num_words, seq_len):\n",
    "\t\tself.data = 'tweets.csv'\n",
    "\t\tself.num_words = num_words\n",
    "\t\tself.seq_len = seq_len\n",
    "\t\tself.vocabulary = None\n",
    "\t\tself.x_tokenized = None\n",
    "\t\tself.x_padded = None\n",
    "\t\tself.x_raw = None\n",
    "\t\tself.y = None\n",
    "\t\t\n",
    "\t\tself.x_train = None\n",
    "\t\tself.x_test = None\n",
    "\t\tself.y_train = None\n",
    "\t\tself.y_test = None\n",
    "\t\t\n",
    "\tdef load_data(self):\n",
    "\t\t# Reads the raw csv file and split into\n",
    "\t\t# sentences (x) and target (y)\n",
    "\t\t\n",
    "\t\tdf = pd.read_csv(self.data)\n",
    "\t\tdf.drop(['id','keyword','location'], axis=1, inplace=True)\n",
    "\t\t\n",
    "\t\tself.x_raw = df['text'].values\n",
    "\t\tself.y = df['target'].values\n",
    "\t\t\n",
    "\tdef clean_text(self):\n",
    "\t\t# Removes special symbols and just keep\n",
    "\t\t# words in lower or upper form\n",
    "\t\t\n",
    "\t\tself.x_raw = [x.lower() for x in self.x_raw]\n",
    "\t\tself.x_raw = [re.sub(r'[^A-Za-z]+', ' ', x) for x in self.x_raw]\n",
    "\t\t\n",
    "\tdef text_tokenization(self):\n",
    "\t\t# Tokenizes each sentence by implementing the nltk tool\n",
    "\t   self.x_raw = [word_tokenize(x) for x in self.x_raw]\n",
    "\t   \n",
    "\tdef build_vocabulary(self):\n",
    "\t\t# Builds the vocabulary and keeps the \"x\" most frequent words\n",
    "\t   self.vocabulary = dict()\n",
    "\t   fdist = nltk.FreqDist()\n",
    "\t   \n",
    "\t   for sentence in self.x_raw:\n",
    "\t      for word in sentence:\n",
    "\t         fdist[word] += 1\n",
    "\t         \n",
    "\t   common_words = fdist.most_common(self.num_words)\n",
    "\t   \n",
    "\t   for idx, word in enumerate(common_words):\n",
    "\t      self.vocabulary[word[0]] = (idx+1)\n",
    "\t      \n",
    "\tdef word_to_idx(self):\n",
    "\t\t# By using the dictionary (vocabulary), it is transformed\n",
    "\t\t# each token into its index based representation\n",
    "\t\t\n",
    "\t   self.x_tokenized = list()\n",
    "\t   \n",
    "\t   for sentence in self.x_raw:\n",
    "\t      temp_sentence = list()\n",
    "\t      for word in sentence:\n",
    "\t         if word in self.vocabulary.keys():\n",
    "\t            temp_sentence.append(self.vocabulary[word])\n",
    "\t      self.x_tokenized.append(temp_sentence)\n",
    "\t      \n",
    "\tdef padding_sentences(self):\n",
    "\t\t# Each sentence which does not fulfill the required len\n",
    "\t\t# it's padded with the index 0\n",
    "\t\t\n",
    "\t   pad_idx = 0\n",
    "\t   self.x_padded = list()\n",
    "\t   \n",
    "\t   for sentence in self.x_tokenized:\n",
    "\t      while len(sentence) < self.seq_len:\n",
    "\t         sentence.insert(len(sentence), pad_idx)\n",
    "\t      self.x_padded.append(sentence)\n",
    "\t   \n",
    "\t   self.x_padded = np.array(self.x_padded)\n",
    "\t   \n",
    "\tdef split_data(self):\n",
    "\t\tself.x_train, self.x_test, self.y_train, self.y_test = train_test_split(self.x_padded, self.y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TextClassifier(nn.ModuleList):\n",
    "\n",
    "\tdef __init__(self, params):\n",
    "\t\tsuper(TextClassifier, self).__init__()\n",
    "\n",
    "\t\t# Parameters regarding text preprocessing\n",
    "\t\tself.seq_len = params.seq_len\n",
    "\t\tself.num_words = params.num_words\n",
    "\t\tself.embedding_size = params.embedding_size\n",
    "\t\t\n",
    "\t\t# Dropout definition\n",
    "\t\tself.dropout = nn.Dropout(0.25)\n",
    "\t\t\n",
    "\t\t# CNN parameters definition\n",
    "\t\t# Kernel sizes\n",
    "\t\tself.kernel_1 = 2\n",
    "\t\tself.kernel_2 = 3\n",
    "\t\tself.kernel_3 = 4\n",
    "\t\tself.kernel_4 = 5\n",
    "\t\t\n",
    "\t\t# Output size for each convolution\n",
    "\t\tself.out_size = params.out_size\n",
    "\t\t# Number of strides for each convolution\n",
    "\t\tself.stride = params.stride\n",
    "\t\t\n",
    "\t\t# Embedding layer definition\n",
    "\t\tself.embedding = nn.Embedding(self.num_words + 1, self.embedding_size, padding_idx=0)\n",
    "\t\t\n",
    "\t\t# Convolution layers definition\n",
    "\t\tself.conv_1 = nn.Conv1d(self.seq_len, self.out_size, self.kernel_1, self.stride)\n",
    "\t\tself.conv_2 = nn.Conv1d(self.seq_len, self.out_size, self.kernel_2, self.stride)\n",
    "\t\tself.conv_3 = nn.Conv1d(self.seq_len, self.out_size, self.kernel_3, self.stride)\n",
    "\t\tself.conv_4 = nn.Conv1d(self.seq_len, self.out_size, self.kernel_4, self.stride)\n",
    "\t\t\n",
    "\t\t# Max pooling layers definition\n",
    "\t\tself.pool_1 = nn.MaxPool1d(self.kernel_1, self.stride)\n",
    "\t\tself.pool_2 = nn.MaxPool1d(self.kernel_2, self.stride)\n",
    "\t\tself.pool_3 = nn.MaxPool1d(self.kernel_3, self.stride)\n",
    "\t\tself.pool_4 = nn.MaxPool1d(self.kernel_4, self.stride)\n",
    "\t\t\n",
    "\t\t# Fully connected layer definition\n",
    "\t\tself.fc = nn.Linear(self.in_features_fc(), 1)\n",
    "\n",
    "\t\t\n",
    "\tdef in_features_fc(self):\n",
    "\t\t'''Calculates the number of output features after Convolution + Max pooling\n",
    "\t\t\t\n",
    "\t\tConvolved_Features = ((embedding_size + (2 * padding) - dilation * (kernel - 1) - 1) / stride) + 1\n",
    "\t\tPooled_Features = ((embedding_size + (2 * padding) - dilation * (kernel - 1) - 1) / stride) + 1\n",
    "\t\t\n",
    "\t\tsource: https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html\n",
    "\t\t'''\n",
    "\t\t# Calcualte size of convolved/pooled features for convolution_1/max_pooling_1 features\n",
    "\t\tout_conv_1 = ((self.embedding_size - 1 * (self.kernel_1 - 1) - 1) / self.stride) + 1\n",
    "\t\tout_conv_1 = math.floor(out_conv_1)\n",
    "\t\tout_pool_1 = ((out_conv_1 - 1 * (self.kernel_1 - 1) - 1) / self.stride) + 1\n",
    "\t\tout_pool_1 = math.floor(out_pool_1)\n",
    "\t\t\n",
    "\t\t# Calcualte size of convolved/pooled features for convolution_2/max_pooling_2 features\n",
    "\t\tout_conv_2 = ((self.embedding_size - 1 * (self.kernel_2 - 1) - 1) / self.stride) + 1\n",
    "\t\tout_conv_2 = math.floor(out_conv_2)\n",
    "\t\tout_pool_2 = ((out_conv_2 - 1 * (self.kernel_2 - 1) - 1) / self.stride) + 1\n",
    "\t\tout_pool_2 = math.floor(out_pool_2)\n",
    "\t\t\n",
    "\t\t# Calcualte size of convolved/pooled features for convolution_3/max_pooling_3 features\n",
    "\t\tout_conv_3 = ((self.embedding_size - 1 * (self.kernel_3 - 1) - 1) / self.stride) + 1\n",
    "\t\tout_conv_3 = math.floor(out_conv_3)\n",
    "\t\tout_pool_3 = ((out_conv_3 - 1 * (self.kernel_3 - 1) - 1) / self.stride) + 1\n",
    "\t\tout_pool_3 = math.floor(out_pool_3)\n",
    "\t\t\n",
    "\t\t# Calcualte size of convolved/pooled features for convolution_4/max_pooling_4 features\n",
    "\t\tout_conv_4 = ((self.embedding_size - 1 * (self.kernel_4 - 1) - 1) / self.stride) + 1\n",
    "\t\tout_conv_4 = math.floor(out_conv_4)\n",
    "\t\tout_pool_4 = ((out_conv_4 - 1 * (self.kernel_4 - 1) - 1) / self.stride) + 1\n",
    "\t\tout_pool_4 = math.floor(out_pool_4)\n",
    "\t\t\n",
    "\t\t# Returns \"flattened\" vector (input for fully connected layer)\n",
    "\t\treturn (out_pool_1 + out_pool_2 + out_pool_3 + out_pool_4) * self.out_size\n",
    "\t\t\n",
    "\t\t\n",
    "\t\t\n",
    "\tdef forward(self, x):\n",
    "\n",
    "\t\t# Sequence of tokes is filterd through an embedding layer\n",
    "\t\tx = self.embedding(x)\n",
    "\t\t\n",
    "\t\t# Convolution layer 1 is applied\n",
    "\t\tx1 = self.conv_1(x)\n",
    "\t\tx1 = torch.relu(x1)\n",
    "\t\tx1 = self.pool_1(x1)\n",
    "\t\t\n",
    "\t\t# Convolution layer 2 is applied\n",
    "\t\tx2 = self.conv_2(x)\n",
    "\t\tx2 = torch.relu((x2))\n",
    "\t\tx2 = self.pool_2(x2)\n",
    "\t\n",
    "\t\t# Convolution layer 3 is applied\n",
    "\t\tx3 = self.conv_3(x)\n",
    "\t\tx3 = torch.relu(x3)\n",
    "\t\tx3 = self.pool_3(x3)\n",
    "\t\t\n",
    "\t\t# Convolution layer 4 is applied\n",
    "\t\tx4 = self.conv_4(x)\n",
    "\t\tx4 = torch.relu(x4)\n",
    "\t\tx4 = self.pool_4(x4)\n",
    "\t\t\n",
    "\t\t# The output of each convolutional layer is concatenated into a unique vector\n",
    "\t\tunion = torch.cat((x1, x2, x3, x4), 2)\n",
    "\t\tunion = union.reshape(union.size(0), -1)\n",
    "\n",
    "\t\t# The \"flattened\" vector is passed through a fully connected layer\n",
    "\t\tout = self.fc(union)\n",
    "\t\t# Dropout is applied\t\t\n",
    "\t\tout = self.dropout(out)\n",
    "\t\t# Activation function is applied\n",
    "\t\tout = torch.sigmoid(out)\n",
    "\t\t\n",
    "\t\treturn out.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Parameters:\n",
    "   # Preprocessing parameeters\n",
    "   seq_len: int = 35\n",
    "   num_words: int = 2000\n",
    "   \n",
    "   # Model parameters\n",
    "   embedding_size: int = 64\n",
    "   out_size: int = 32\n",
    "   stride: int = 2\n",
    "   \n",
    "   # Training parameters\n",
    "   epochs: int = 10\n",
    "   batch_size: int = 12\n",
    "   learning_rate: float = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class DatasetMaper(Dataset):\n",
    "\n",
    "\tdef __init__(self, x, y):\n",
    "\t\tself.x = x\n",
    "\t\tself.y = y\n",
    "\t\t\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.x)\n",
    "\t\t\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\treturn self.x[idx], self.y[idx]\n",
    "\n",
    "class Run:\n",
    "\t'''Training, evaluation and metrics calculation'''\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef train(model, data, params):\n",
    "\t\t\n",
    "\t\t# Initialize dataset maper\n",
    "\t\ttrain = DatasetMaper(data['x_train'], data['y_train'])\n",
    "\t\ttest = DatasetMaper(data['x_test'], data['y_test'])\n",
    "\t\t\n",
    "\t\t# Initialize loaders\n",
    "\t\tloader_train = DataLoader(train, batch_size=params.batch_size)\n",
    "\t\tloader_test = DataLoader(test, batch_size=params.batch_size)\n",
    "\t\t\n",
    "\t\t# Define optimizer\n",
    "\t\toptimizer = optim.RMSprop(model.parameters(), lr=params.learning_rate)\n",
    "\t\t\n",
    "\t\t# Starts training phase\n",
    "\t\tfor epoch in range(params.epochs):\n",
    "\t\t\t# Set model in training model\n",
    "\t\t\tmodel.train()\n",
    "\t\t\tpredictions = []\n",
    "\t\t\t# Starts batch training\n",
    "\t\t\tfor x_batch, y_batch in loader_train:\n",
    "\t\t\t\n",
    "\t\t\t\ty_batch = y_batch.type(torch.FloatTensor)\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Feed the model\n",
    "\t\t\t\ty_pred = model(x_batch)\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Loss calculation\n",
    "\t\t\t\tloss = F.binary_cross_entropy(y_pred, y_batch)\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Clean gradientes\n",
    "\t\t\t\toptimizer.zero_grad()\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Gradients calculation\n",
    "\t\t\t\tloss.backward()\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Gradients update\n",
    "\t\t\t\toptimizer.step()\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Save predictions\n",
    "\t\t\t\tpredictions += list(y_pred.detach().numpy())\n",
    "\t\t\t\n",
    "\t\t\t# Evaluation phase\n",
    "\t\t\ttest_predictions = Run.evaluation(model, loader_test)\n",
    "\t\t\t\n",
    "\t\t\t# Metrics calculation\n",
    "\t\t\ttrain_accuary = Run.calculate_accuray(data['y_train'], predictions)\n",
    "\t\t\ttest_accuracy = Run.calculate_accuray(data['y_test'], test_predictions)\n",
    "\t\t\tprint(\"Epoch: %d, loss: %.5f, Train accuracy: %.5f, Test accuracy: %.5f\" % (epoch+1, loss.item(), train_accuary, test_accuracy))\n",
    "\t\t\t\n",
    "\t@staticmethod\n",
    "\tdef evaluation(model, loader_test):\n",
    "\t\t\n",
    "\t\t# Set the model in evaluation mode\n",
    "\t\tmodel.eval()\n",
    "\t\tpredictions = []\n",
    "\t\t\n",
    "\t\t# Starst evaluation phase\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\tfor x_batch, y_batch in loader_test:\n",
    "\t\t\t\ty_pred = model(x_batch)\n",
    "\t\t\t\tpredictions += list(y_pred.detach().numpy())\n",
    "\t\treturn predictions\n",
    "\t\t\n",
    "\t@staticmethod\n",
    "\tdef calculate_accuray(grand_truth, predictions):\n",
    "\t\t# Metrics calculation\n",
    "\t\ttrue_positives = 0\n",
    "\t\ttrue_negatives = 0\n",
    "\t\tfor true, pred in zip(grand_truth, predictions):\n",
    "\t\t\tif (pred >= 0.5) and (true == 1):\n",
    "\t\t\t\ttrue_positives += 1\n",
    "\t\t\telif (pred < 0.5) and (true == 0):\n",
    "\t\t\t\ttrue_negatives += 1\n",
    "\t\t\telse:\n",
    "\t\t\t\tpass\n",
    "\t\t# Return accuracy\n",
    "\t\treturn (true_positives+true_negatives) / len(grand_truth)\n",
    "\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Controller(Parameters):\n",
    "\t\n",
    "\tdef __init__(self):\n",
    "\t\t# Preprocessing pipeline\n",
    "\t\tself.data = self.prepare_data(Parameters.num_words, Parameters.seq_len)\n",
    "\t\t\n",
    "\t\t# Initialize the model\n",
    "\t\tself.model = TextClassifier(Parameters)\n",
    "\t\t\n",
    "\t\t# Training - Evaluation pipeline\n",
    "\t\tRun().train(self.model, self.data, Parameters)\n",
    "\t\t\n",
    "\t\t\n",
    "\t@staticmethod\n",
    "\tdef prepare_data(num_words, seq_len):\n",
    "\t\t# Preprocessing pipeline\n",
    "\t\tpr = Preprocessing(num_words, seq_len)\n",
    "\t\tpr.load_data()\n",
    "\t\tpr.clean_text()\n",
    "\t\tpr.text_tokenization()\n",
    "\t\tpr.build_vocabulary()\n",
    "\t\tpr.word_to_idx()\n",
    "\t\tpr.padding_sentences()\n",
    "\t\tpr.split_data()\n",
    "\n",
    "\t\treturn {'x_train': pr.x_train, 'y_train': pr.y_train, 'x_test': pr.x_test, 'y_test': pr.y_test}\n",
    "\t\t\n",
    "# if __name__ == '__main__':\n",
    "# \tcontroller = Controller()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, loss: 0.47676, Train accuracy: 0.57278, Test accuracy: 0.65231\n",
      "Epoch: 2, loss: 0.31520, Train accuracy: 0.66124, Test accuracy: 0.70378\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-de96efd4658a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcontroller\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mController\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-2965a93986c1>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                 \u001b[0;31m# Training - Evaluation pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m                 \u001b[0mRun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mParameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-688b23beaf28>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, data, params)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                                 \u001b[0;31m# Gradients calculation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m                                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m                                 \u001b[0;31m# Gradients update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "controller = Controller()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
